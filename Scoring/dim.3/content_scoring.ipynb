{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f92bfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a5f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54036d7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "CORE_KEYWORDS = {\n",
    "    '鲜芋仙', 'Meet Fresh', 'MeetFresh', '台湾美食', '甜品', \n",
    "    '芋圆', 'taro', '仙草', 'grass jelly', '奶茶', 'milk tea',\n",
    "    '豆花', 'tofu pudding', '奶刨冰', 'milked shaved ice',\n",
    "    '红豆汤', 'purple rice soup', '紫米粥', 'red bean soup',\n",
    "    '2001 Coit Rd', 'Park Pavillion Center', '(972) 596-6088'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549af5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentScorer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.users = df['user_id'].unique()\n",
    "        self._prepare_models()\n",
    "        \n",
    "    def _prepare_models(self):\n",
    "        \"\"\"预加载所有模型\"\"\"\n",
    "        # 原创性检测模型\n",
    "        self.tfidf = TfidfVectorizer(max_features=6000)\n",
    "        self.simcse = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        \n",
    "        # 情感分析模型\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.bert_model.eval()\n",
    "        \n",
    "    def calculate_all_scores(self):\n",
    "        \"\"\"主流程：计算所有分数\"\"\"\n",
    "        # 预处理\n",
    "        self._preprocess()\n",
    "        \n",
    "        # 计算各维度分数\n",
    "        self._calc_originality()\n",
    "        self._calc_vertical()\n",
    "        self._calc_sentiment()\n",
    "        self._calc_keyword()\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        \"\"\"数据预处理\"\"\"\n",
    "        # 转换时间格式\n",
    "        self.df['elapsed_days'] = self.df['elapsed_time']\n",
    "        \n",
    "        # 构建用户笔记映射\n",
    "        self.user_notes = self.df.groupby('user_id')['text'].apply(list)\n",
    "        \n",
    "    # ======================\n",
    "    # 1. 文本原创性计算（用户维度）\n",
    "    # ======================\n",
    "    def _calc_originality(self):\n",
    "        \"\"\"原创性评分\"\"\"\n",
    "        all_texts = self.df['text'].tolist()\n",
    "        \n",
    "        # TF-IDF相似度\n",
    "        tfidf_matrix = self.tfidf.fit_transform(all_texts)\n",
    "        tfidf_sim = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # SimCSE相似度\n",
    "        embeddings = self.simcse.encode(all_texts, show_progress_bar=True)\n",
    "        simcse_sim = cosine_similarity(embeddings)\n",
    "        \n",
    "        # 取最大值并计算得分\n",
    "        max_sim = np.maximum(tfidf_sim.max(axis=1), simcse_sim.max(axis=1))\n",
    "        self.df['originality'] = (1 - max_sim) * 25\n",
    "        \n",
    "    # ======================\n",
    "    # 2. 垂直领域分布（使用tag_list）\n",
    "    # ======================\n",
    "    def _calc_vertical(self):\n",
    "        \"\"\"垂直领域评分\"\"\"\n",
    "        target_tags = {'美食', '探店', '餐饮', '餐厅', '美食探店'}\n",
    "        \n",
    "        def _calc_similarity(tags):\n",
    "            user_tags = set(tags.split(','))\n",
    "            intersection = user_tags & target_tags\n",
    "            return len(intersection) / len(target_tags) if target_tags else 0\n",
    "        \n",
    "        self.df['vertical'] = self.df['tag_list'].apply(\n",
    "            lambda x: min(_calc_similarity(x)*25, 25)\n",
    "        )\n",
    "        \n",
    "    # ======================\n",
    "    # 3. 情感强度计算\n",
    "    # ======================\n",
    "    def _calc_sentiment(self):\n",
    "        \"\"\"情感评分\"\"\"\n",
    "        # 时间衰减系数\n",
    "        decay = np.exp(-0.05 * self.df['elapsed_days'] / 7)\n",
    "        \n",
    "        # 情感值计算\n",
    "        self.df['sentiment'] = self.df['text'].apply(\n",
    "            lambda x: self._get_bert_sentiment(x)\n",
    "        ) * decay\n",
    "        \n",
    "        # 标准化\n",
    "        p90 = self.df['sentiment'].quantile(0.9)\n",
    "        self.df['sentiment'] = (self.df['sentiment'] / p90 * 25).clip(0, 25)\n",
    "        \n",
    "    def _get_bert_sentiment(self, text):\n",
    "        \"\"\"BERT情感分析\"\"\"\n",
    "        inputs = self.bert_tokenizer(text[:512], return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "        return outputs.last_hidden_state.mean().item()\n",
    "    \n",
    "    # ======================\n",
    "    # 4. 关键词覆盖\n",
    "    # ======================\n",
    "    def _calc_keyword(self):\n",
    "        \"\"\"关键词评分\"\"\"\n",
    "        # 时间衰减\n",
    "        decay = np.exp(-0.05 * self.df['elapsed_days'] / 7)\n",
    "        \n",
    "        # 关键词计数\n",
    "        pattern = re.compile('|'.join(re.escape(kw) for kw in CORE_KEYWORDS), flags=re.IGNORECASE)\n",
    "        self.df['kw_count'] = self.df['text'].apply(\n",
    "            lambda x: len(pattern.findall(x))\n",
    "        )\n",
    "        \n",
    "        # 单篇得分\n",
    "        single_score = np.maximum(1, self.df['kw_count'] / 5) * decay\n",
    "        \n",
    "        # 用户级计算\n",
    "        user_scores = self.df.groupby('user_id').apply(self._user_keyword_score)\n",
    "        self.df = self.df.merge(user_scores, on='user_id')\n",
    "        \n",
    "    def _user_keyword_score(self, group):\n",
    "        \"\"\"用户维度关键词计算\"\"\"\n",
    "        # 基准值计算\n",
    "        p90 = group['single_score'].quantile(0.9)\n",
    "        cover_rate = (group['kw_count'] > 0).mean()\n",
    "        \n",
    "        # 综合得分\n",
    "        score = (group['single_score'].mean() / p90 * 0.7 + cover_rate / 0.5 * 0.3) * 25\n",
    "        return pd.Series({'keyword': score.clip(0, 25)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c205b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3c1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
