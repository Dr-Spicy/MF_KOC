# -*- coding: utf-8 -*-

"""
æ··åˆè¯­å¢ƒæ–‡æœ¬å¤„ç†æ¨¡å— (XHS Mixed Language Processor)

- ä¸“ä¸ºå¤„ç†å°çº¢ä¹¦å¹³å°ä¸Šçš„ä¸­è‹±æ··åˆæ–‡æœ¬è€Œè®¾è®¡çš„class, XHSMixedLanguageProcessorï¼Œå…¶æµç¨‹åŒ…æ‹¬:               
    # 1. åŸºç¡€æ¸…æ´—(å»é™¤HTMLæ ‡ç­¾ã€URLã€ç‰¹æ®Šç¬¦å·ç­‰, å…¨è§’è½¬åŠè§’, ç©ºæ ¼æ ‡å‡†åŒ–)
    text = self.basic_clean(text)

    # 2. ç¤¾äº¤åª’ä½“ç‰¹å®šæ¸…æ´— (å»é™¤è¯é¢˜æ ‡ç­¾ã€@æåŠã€Emojiçš„demojize, å¤„ç†å°çº¢ä¹¦ç‰¹å®šæ ‡ç­¾å’Œè¡¨æƒ…åŒ…)
    text = self.social_media_clean(text)

    # 3. ä¸­è‹±è¯­å¢ƒä¼˜åŒ–ï¼ˆç”¨langidåšè¯­è¨€æ£€æµ‹, å¯¹æ‰¹é‡å¤„ç†è¶…è¿‡100æ¡ä¸”æ–‡æœ¬ä¸­æ–‡å«é‡ä½äº50%æ—¶ç”¨deep_translator APIç¿»è¯‘ï¼‰
    if enable_translation and is_batch_large:
        text = self.language_optimize(text, protected_terms=self.protected_terms)

    # 4. è¯­ä¹‰æ¸…æ´— (jiebaä¸­æ–‡åˆ†è¯, æ ¹æ®TF-IDFå»é™¤å™ªå£°è¯, å¹¶ä¿æŠ¤é¢†åŸŸå…³é”®è¯ï¼‰
    text = self.chinese_semantic_clean([text])[0]

- ä½¿ç”¨æ–¹æ³•:
    - å•æ¡æ–‡æœ¬å¤„ç†: Class.process_text(text)
    - æ‰¹é‡æ–‡æœ¬å¤„ç†: Class.batch_process([text1, text2, ...]) # æ–‡æœ¬æ•°é‡>100æä¾›ç¿»è¯‘é€‰é¡¹
    - æŸ¥çœ‹å¤„ç†ç»Ÿè®¡ä¿¡æ¯: Class.get_stats()

- å¤„ç†çš„äº®ç‚¹ï¼š
    - é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ•ˆç‡
    - æ‰¹é‡å¤„ç†å¾…ç¿»è¯‘å†…å®¹, å¹¶æ·»åŠ ç¼“å­˜ä»¥å‡å°‘APIè°ƒç”¨
    - ä½¿ç”¨ThreadPoolExecutorå¹¶è¡Œæ‰§è¡ŒI/Oå¯†é›†å‹ä»»åŠ¡æ¥æé«˜æ‰¹é‡å¤„ç†æ€§èƒ½

XHS Mixed Language Processor

- Overview
A specialized class designed for processing mixed Chinese-English text from Xiaohongshu (XHS). 

- Workflows:
    - Basic Cleaning: Remove HTML tags, URLs, and special symbols; convert full-width to half-width; normalize spaces.
    - Social Media Cleaning: Remove hashtags, @mentions, demojize emojis, and handle XHS-specific tags.
    - Language Optimization (using langid for language detection and deep_translator API for sentences with batch size > 100 and Chinese content < 50%):
    - Semantic Cleaning: Use jieba for Chinese tokenization, remove noise words based on TF-IDF, and preserve domain-specific keywords.

- Usage
    - Single text processing: Class.process_text(text)
    - Batch processing: Class.batch_process([text1, text2, ...]) # translation option for >100 texts
    - View processing stats: Class.get_stats()

- Highlights
    - Precompiled regex for efficiency
    - Batch translation requests with caches to reduce API calls
    - Parallel I/O tasks with ThreadPoolExecutor for better batch performance
"""

import os
import logging
import time
import re
import emoji
from typing import Literal, List, Set, Dict, Union, Optional, Tuple
import json
from langid import classify
import jieba 
from deep_translator import GoogleTranslator
from functools import lru_cache
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("MixedLanguageProcessor")
jieba_logger = logging.getLogger('jieba')
jieba_logger.setLevel(logging.INFO)  # Suppress DEBUG messages

# Precompile regex patterns for efficiency
ZH_CHAR_PATTERN = re.compile(r'[\u4e00-\u9fff]')  # ä¸­æ–‡å­—ç¬¦æ£€æµ‹
NON_ZH_PATTERN = re.compile(r'[a-zA-Z]')  # æ‹‰ä¸å­—æ¯æ£€æµ‹
SPLIT_PATTERN = re.compile(r'([ã€‚ï¼ï¼Ÿ?!.])')  # åˆ†å¥æ­£åˆ™
URL_PATTERN = re.compile(
    r'(?:https?://)?(?:[a-zA-Z0-9\u4e00-\u9fff-]+\.)+[a-zA-Z]{2,}'
    r'(?:/\S*)?(?:\?\S*)?(?:#\S*)?',
    flags=re.IGNORECASE
)
TOPIC_PATTERN = re.compile(r'#([^#]+)#')
MENTION_PATTERN = re.compile(r'@[\w\u4e00-\u9fff-]+')  # æ”¯æŒä¸­è‹±æ–‡ç”¨æˆ·å
XIAOHONGSHU_TAG_PATTERN = re.compile(r'\[(è¯é¢˜|è¡¨æƒ…|åœ°ç‚¹)\s*[^\]]*\]')
HTML_TAG_PATTERN = re.compile(r'<[^>]+>')
WHITESPACE_PATTERN = re.compile(r'[\t\n\u3000]')
MULTI_SPACE_PATTERN = re.compile(r'\s+')

# Define the main class for mixed language processing
class XHSMixedLanguageProcessor:
    """æ··åˆè¯­å¢ƒæ–‡æœ¬å¤„ç†å™¨ï¼Œä¸“ä¸ºå¤„ç†ä¸­è‹±æ··åˆçš„ç¤¾äº¤åª’ä½“å†…å®¹è®¾è®¡
    å•æ¡æ–‡æœ¬å¤„ç†ä¸æä¾›ç¿»è¯‘åŠŸèƒ½ï¼Œæ‰¹é‡å¤„ç†è¶…è¿‡100æ¡æ—¶æ‰å¯ç”¨ç¿»è¯‘"""
    
    def __init__(self, cache_size=1000, max_workers=12):
        """        
        Args:
            cache_size: ç¿»è¯‘ç¼“å­˜çš„æœ€å¤§æ¡ç›®æ•°
            max_workers: å¹¶è¡Œå¤„ç†çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers = max_workers
        self._translator = GoogleTranslator(source='auto', target='zh-CN')
        self.translation_cache = {}
        self.cache_size = cache_size
        self.cache_hits = 0
        self.translation_requests = 0

        # Translation management metrics
        self._success_streak = 0
        self._failure_streak = 0
        self._last_error_time = 0
        
        # protected_terms and domain_keywords
        self.domain_keywords = {
        'é²œèŠ‹ä»™', 'Meet Fresh', 'MeetFresh', 'å°æ¹¾ç¾é£Ÿ', 'ç”œå“', 
        'èŠ‹åœ†', 'taro ball', 'èŠ‹å¤´', 'taro', 'ä»™è‰', 'grass jelly', 'å¥¶èŒ¶', 'milk tea',
        'è±†èŠ±', 'tofu pudding', 'ç‰›å¥¶å†°', 'milked shaved ice', 'å¥¶åˆ¨å†°',
        'çº¢è±†æ±¤', 'purple rice soup', 'ç´«ç±³ç²¥', 'red bean soup',
        '2001 Coit Rd', 'Park Pavillion Center', '(972) 596-6088',
        'é¤å…', 'é¤é¦†', 'ç¾é£Ÿ', 'å°æ¹¾å°åƒ', 'å°æ¹¾ç”œå“', 'å†°æ¿€å‡Œ',
        "VIP", "AI", "DFW", "Dallas", "è¾¾æ‹‰æ–¯", 'å«ç”Ÿ', 'æ•´æ´', 'å¹²å‡€', 'æœåŠ¡', 'æ€åº¦', 
        'å¥½è¯„', 'æ¨è', 'åˆ†äº«', 'ä½“éªŒ', 'ç¾å‘³', 'å¥½åƒ', 'å¥½å–', 'æ–°é²œ', 'æ­£å®—',
        'ç‰¹è‰²', 'å£å‘³', 'èœå“', 'ç¯å¢ƒ', 'æ°›å›´', 'ä»·æ ¼', 'å®æƒ ', 'åˆ’ç®—', 'ä¼˜æƒ ', 'æŠ˜æ‰£', 
        'æ´»åŠ¨', 'å¥—é¤', 'åˆ†é‡', 'è¶³é‡', 'æ»¡æ„', 'ç‚¹èµ', 'åˆ˜ä¸€æ‰‹', 'å°é¾™è™¾', 'ç«é”…',
        'çƒ§çƒ¤', 'ä¸²ä¸²é¦™', 'éº»è¾£çƒ«', 'å°åƒ', 'ç‚¸é¸¡', 'æ±‰å ¡', 'æŠ«è¨', 'å¯¿å¸', 'åˆºèº«',
        'å¥¶æ˜”', 'æµ·é²œ', 'é±¼', 'è‚‰', 'è”¬èœ', 'é”…åº•', 'è˜¸æ–™', 'è°ƒæ–™', 'é…±æ±', 'é…èœ',
        }
        self.protected_terms = {
        'Meet Fresh', 'MeetFresh', 'taro ball', 'taro', 'grass jelly', 'milk tea',
        'tofu pudding', 'milked shaved ice', 'purple rice soup', 'red bean soup',
        '2001 Coit Rd', 'Park Pavillion Center', "VIP", "AI", "DFW", "Dallas",
        }

    def register_domain_keywords_with_jieba(self, keywords):
        """Register domain keywords with jieba to improve segmentation"""
        for keyword in keywords:
            if isinstance(keyword, str) and len(keyword) >= 2:
                # The higher frequency (third parameter) ensures jieba treats this as a single word
                jieba.add_word(keyword, freq=10000)
        
    def fullwidth_to_halfwidth(self, text: str) -> str:
        """å…¨è§’è½¬åŠè§’ï¼ˆä¿ç•™ï¿¥ç¬¦å·ï¼‰"""
        translation_table = str.maketrans({
        'ï¼': '!', '\u201c': '"', '\u201d': '"', '\u2018': "'", '\u2019': "'",
        'ã€': ',', 'ï¼Œ': ',', 'ï¼›': ';', 'ï¼š': ':', 'ï¼Ÿ': '?',
        'ã€Š': '<', 'ã€‹': '>', 'ã€': '[', 'ã€‘': ']', 'Â·': '.',
        'ï½': '~', 'â€”': '-', 'ï¼ˆ': '(', 'ï¼‰': ')', 'ã€€': ' '
        })
        return text.translate(translation_table)

    def normalize_punctuation(self, text: str) -> str:
        """ç¬¦å·æ ‡å‡†åŒ–ï¼ˆä¿ç•™emojiå’Œé‡è¦ç¬¦å·ï¼‰"""
        # å®šä¹‰ä¿ç•™ç¬¦å·é›†
        keep_symbols = {"'", '"', ',', '.', ':', ';', '!', '?', '-', 
                        '(', ')', '<', '>', '[', ']', '&', '#', '@',
                        '%', '$', 'ï¿¥', '/', '=', '+', '~', '^'}
        
        # å­—ç¬¦çº§å¤„ç† 
        cleaned_chars = []
        for char in text:
            # ä¿ç•™æ¡ä»¶ï¼šå­—æ¯æ•°å­—/æ±‰å­—/keep_symbols/emoji
            if (char.isalnum() or
                '\u4e00' <= char <= '\u9fff' or
                char in keep_symbols or
                emoji.is_emoji(char)):
                cleaned_chars.append(char)
            else:
                cleaned_chars.append(' ')
        
        return ''.join(cleaned_chars)

    def remove_urls(self, text: str) -> str:
        """é€‚é…ä¸­æ–‡åŸŸåçš„URLç§»é™¤"""
        return URL_PATTERN.sub('', text)

    def basic_clean(self, text: str) -> str:
        """
        å¯¹æ–‡æœ¬è¿›è¡ŒåŸºç¡€å±‚æ¸…æ´—ï¼ŒåŒ…æ‹¬å»é™¤HTMLæ ‡ç­¾ã€URLã€ç‰¹æ®Šç¬¦å·å¤„ç†, ç©ºæ ¼æ ‡å‡†åŒ–ç­‰ã€‚
        """
        if not text:
            return ""
            
        # æ›¿æ¢æ‰€æœ‰ç©ºç™½ç¬¦ï¼ˆå«å°çº¢ä¹¦å¸¸è§çš„å…¨è§’ç©ºæ ¼\u3000ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ï¼‰
        text = WHITESPACE_PATTERN.sub(' ', text)
        # HTMLæ ‡ç­¾ç§»é™¤
        text = HTML_TAG_PATTERN.sub('', text)
        # URLç§»é™¤, é€‚é…ä¸­æ–‡åŸŸå
        text = self.remove_urls(text)
        # å…¨è§’è½¬åŠè§’ï¼ˆä¿ç•™å…¨è§’ï¿¥ï¼‰
        text = self.fullwidth_to_halfwidth(text)
        # ç¬¦å·æ ‡å‡†åŒ–
        text = self.normalize_punctuation(text)
        # æ ‡å‡†åŒ–åˆå¹¶è¿ç»­ç©ºæ ¼
        text = MULTI_SPACE_PATTERN.sub(' ', text).strip()
        return text

    def social_media_clean(self, text: str, strategy='demojize') -> str:
        """
        ç¤¾äº¤åª’ä½“æ–‡æœ¬æ¸…æ´—ï¼Œä¸»è¦é’ˆå¯¹å°çº¢ä¹¦å¹³å°çš„ç‰¹å®šæ ¼å¼å’Œç¬¦å·è¿›è¡Œå¤„ç†ã€‚
        """
        if not text:
            return ""
            
        # ç§»é™¤è¯é¢˜æ ‡ç­¾ä½†ä¿ç•™å…³é”®è¯ï¼ˆå¦‚ #è¾¾æ‹‰æ–¯ç¾é£Ÿ# â†’ è¾¾æ‹‰æ–¯ç¾é£Ÿï¼‰
        text = TOPIC_PATTERN.sub(r'\1', text)
        
        # ç§»é™¤@æåŠ(åŒ…å«å…¶å˜ä½“, å¦‚@å°çº¢ä¹¦ç”¨æˆ·)
        text = MENTION_PATTERN.sub('', text)
        
        # è½¬æ¢Emojiï¼ˆå¯é€‰ç­–ç•¥ï¼‰
        if strategy == 'remove':
            text = emoji.replace_emoji(text, replace='')
        elif strategy == 'demojize':
            # å°†emojiè½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼ˆå¦‚:ğŸ˜€ â†’ :grinning_face:ï¼‰
            text = emoji.demojize(text, delimiters=('__EMOJI_', '__'))
        
        # å¤„ç†å°çº¢ä¹¦ç‰¹æœ‰çš„æ–¹æ‹¬å·æ ‡ç­¾ï¼ˆåˆ é™¤ç³»ç»Ÿæ ‡ç­¾, å¦‚[è¯é¢˜]â†’'')
        text = XIAOHONGSHU_TAG_PATTERN.sub('', text)

        # ç§»é™¤å°çº¢ä¹¦è¡¨æƒ…åŒ…æ ‡ç­¾ï¼Œå¦‚[ç¬‘å“­R]â†’''
        text = re.sub(r'\[[^\]]*R\]', '', text)
        
        return text.strip()

    def ratio_of_chinese(self, text: str) -> float:
        """è¿”å› text ä¸­ï¼ˆUnicodeèŒƒå›´4E00-9FFFï¼‰æ±‰å­—çš„å æ¯”ã€‚"""
        if not text:
            return 1.0
        zh_chars = ZH_CHAR_PATTERN.findall(text)
        return len(zh_chars) / len(text) if len(text) > 0 else 0.0

    def mask_protected_terms(self, text: str, protected_terms: set) -> (str, dict):
        """
        å°† text ä¸­å‡ºç°çš„ä¿æŠ¤è¯ç”¨å ä½ç¬¦æ›¿æ¢ï¼Œå¹¶è¿”å›æ›¿æ¢åçš„æ–‡æœ¬ä»¥åŠå ä½ç¬¦æ˜ å°„å­—å…¸ã€‚
        ä½¿ç”¨æ›´éš¾è¢«ç¿»è¯‘APIä¿®æ”¹çš„å ä½ç¬¦æ ¼å¼: __TERM_0__
        """
        if not protected_terms:
            return text, {}
        
        # æŒ‰æœ¯è¯­é•¿åº¦é™åºæ’åˆ—ï¼ˆä¼˜å…ˆåŒ¹é…é•¿è¯ï¼‰
        sorted_terms = sorted(protected_terms, key=lambda x: len(str(x)), reverse=True)
        
        # ä¸ä½¿ç”¨word boundaryï¼Œè€Œæ˜¯ç›´æ¥åŒ¹é…æ•´ä¸ªè¯ï¼Œæé«˜åŒ¹é…ç²¾åº¦
        pattern = re.compile(
            '(' + '|'.join(map(re.escape, sorted_terms)) + ')', 
            flags=re.IGNORECASE
        )
        
        placeholder_map = {}
        idx = 0
        
        def _repl(m):
            nonlocal idx
            # ä½¿ç”¨æ›´ç¨³å®šçš„å ä½ç¬¦æ ¼å¼
            placeholder = f"__TERM_{idx}__"
            placeholder_map[placeholder] = m.group(0)
            idx += 1
            return placeholder
        
        return pattern.sub(_repl, text), placeholder_map

    def unmask_protected_terms(self, text: str, placeholder_map: dict) -> str:
        """å°†ç¿»è¯‘åæ–‡æœ¬ä¸­çš„å ä½ç¬¦æ¢å¤æˆåŸå§‹è‹±æ–‡æœ¯è¯­ï¼Œæ”¯æŒä¸åŒºåˆ†å¤§å°å†™"""
        if not placeholder_map or not text:
            return text
            
        result = text
        for placeholder, original in placeholder_map.items():
            # åˆ›å»ºä¸åŒºåˆ†å¤§å°å†™çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
            pattern = re.compile(re.escape(placeholder), re.IGNORECASE)
            # æ›¿æ¢æ‰€æœ‰åŒ¹é…
            result = pattern.sub(original, result)
        return result
        
    def _update_cache(self, key: str, value: str) -> None:
        """å°†ç¿»è¯‘ç»“æœæ·»åŠ åˆ°ç¼“å­˜ï¼Œç®¡ç†ç¼“å­˜å¤§å°"""
        # Skip caching for very long texts to save memory
        if len(key) > 1000:
            return
            
        # If cache is full, remove least recently used items (20% of capacity)
        if len(self.translation_cache) >= self.cache_size:
            # Remove oldest 20% of entries
            items_to_remove = max(1, int(self.cache_size * 0.2))
            for _ in range(items_to_remove):
                if self.translation_cache:
                    self.translation_cache.pop(next(iter(self.translation_cache)))
                    
        # Store in cache
        self.translation_cache[key] = value
        
    def batch_translate(self, texts: List[str]) -> List[str]:
        """æ‰¹é‡ç¿»è¯‘å‡½æ•°ï¼Œåˆ©ç”¨ç¼“å­˜å¹¶ä½¿ç”¨ä»¤ç‰Œæ¡¶é™æµå’Œæ™ºèƒ½é‡è¯•ç­–ç•¥, ä»…åœ¨å¤„ç†100æ¡ä»¥ä¸Šæ–‡æœ¬æ—¶ä½¿ç”¨"""
        if not texts:
            return []
        
        # Filter out empty texts or numeric-only texts
        to_translate = []
        indices = []
        results = [""] * len(texts)
        
        # First check cache for all texts
        for i, text in enumerate(texts):
            if not text or text.strip().isdigit():
                results[i] = text
            elif text in self.translation_cache:
                # Cache hit
                results[i] = self.translation_cache[text]
                self.cache_hits += 1
            else:
                # Cache miss, need to translate
                if len(text) > 5000:
                    text = text[:4900]
                to_translate.append(text)
                indices.append(i)
        
        if not to_translate:
            return results
        
        # Remove duplicates to minimize API calls (preserving original indices)
        unique_texts = []
        text_to_indices = {}
        for idx, text in zip(indices, to_translate):
            if text not in text_to_indices:
                text_to_indices[text] = [idx]
                unique_texts.append(text)
            else:
                text_to_indices[text].append(idx)
        
        # Translation parameters based on past performance
        success_streak = self._success_streak
        failure_streak = self._failure_streak
        last_error_time = self._last_error_time
        current_time = time.time()
        
        # Adaptive batch size - decrease after failures, increase after successes
        base_batch_size = 5
        if failure_streak > 0:
            # Reduce batch size after failures
            batch_size = max(1, min(3, base_batch_size - failure_streak))
            
            # Apply longer cooldown after frequent failures
            if current_time - last_error_time < 30 and failure_streak > 2:
                logger.info(f"Cooling down translation API for 5 seconds after {failure_streak} failures")
                time.sleep(5)
        else:
            # Gradually increase batch size after successful operations
            batch_size = min(8, base_batch_size + min(3, success_streak // 10))
        
        # Process unique texts in batches
        unique_results = {}
        total_unique = len(unique_texts)
        processed = 0
        
        for i in range(0, total_unique, batch_size):
            batch = unique_texts[i:i+batch_size]
            remaining = total_unique - processed
            logger.debug(f"Translating batch {i//batch_size + 1}, size={len(batch)}, remaining={remaining}")
            
            # Exponential backoff for retries
            max_retries = 3
            retry_count = 0
            success = False
            
            while not success and retry_count <= max_retries:
                try:
                    # Apply rate limiting with adaptive delay
                    delay = 0.1 * (2 ** retry_count) if retry_count > 0 else 0
                    if delay > 0:
                        time.sleep(delay)
                    
                    # Perform translation
                    self.translation_requests += len(batch)
                    translated = self._translator.translate_batch(batch)
                    
                    # Update results and cache
                    for txt, trans in zip(batch, translated):
                        unique_results[txt] = trans
                        self._update_cache(txt, trans)
                    
                    # Update success metrics
                    self._success_streak = success_streak + 1
                    self._failure_streak = 0
                    success = True
                    processed += len(batch)
                    
                except Exception as e:
                    retry_count += 1
                    self._last_error_time = time.time()
                    
                    if "too many requests" in str(e).lower():
                        # Rate limit hit - apply exponential backoff
                        self._failure_streak = failure_streak + 1
                        self._success_streak = 0
                        wait_time = min(30, 1 * (2 ** retry_count))  # Cap at 30 seconds
                        logger.warning(f"Hit rate limit, retrying in {wait_time}s (attempt {retry_count}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"Translation error: {e}, retrying in {delay}s (attempt {retry_count}/{max_retries})")
                        if retry_count >= max_retries:
                            # Fall back to original texts after max retries
                            for txt in batch:
                                unique_results[txt] = txt
                            processed += len(batch)
            
            # Add small delay between batches to avoid rate limits
            if i + batch_size < total_unique:
                time.sleep(0.2)
        
        # Map unique results back to all requested texts
        for text, text_indices in text_to_indices.items():
            for idx in text_indices:
                results[idx] = unique_results.get(text, text)
        
        return results

    def language_optimize(
        self,
        text: str,
        threshold: float = 0.5,
        protected_terms: set = None,
        enable_translation: bool = True
    ) -> str:
        """
        è´Ÿè´£å¤„äºåŒè¯­ç¯å¢ƒçš„æ–‡æœ¬ä¼˜åŒ–, ç»Ÿä¸€çš„è¯­è¨€ä¼˜åŒ–æ–¹æ³•ï¼Œæ”¯æŒå•æ–‡æœ¬å’Œæ‰¹å¤„ç†æ¨¡å¼
        ä¿®æ”¹ä¸ºå…ˆæ£€æµ‹æ•´ä¸ªnoteçš„ä¸­æ–‡å«é‡ï¼Œåªæœ‰å½“æ•´ä½“ä¸­æ–‡å«é‡ä½äºé˜ˆå€¼æ—¶æ‰è¿›è¡Œç¿»è¯‘å¤„ç†
        1. å¿«é€Ÿåˆ†å¥å¤„ç†
        2. è¯­è¨€æ£€æµ‹è¿‡æ»¤éœ€è¦ç¿»è¯‘çš„å¥å­
        3. åˆå¹¶é‡ç»„å¤„ç†ç»“æœ
        """
        if not enable_translation or not text:
            return text

        if protected_terms is None:
            protected_terms = self.protected_terms

        # Skip if no non-Chinese characters
        if not NON_ZH_PATTERN.search(text):
            return text
        
        # Split into segments more efficiently
        segments = SPLIT_PATTERN.split(text)
        
        # Process sentences and separators in one pass
        sentences = segments[::2]  # Even indices are sentences
        separators = segments[1::2] if len(segments) > 1 else []  # Odd indices are separators
        
        # Single collection for all translation tasks
        to_translate = []
        indices = []
        ph_maps = []
        result_sentences = [""] * len(sentences)
        
        # Identify sentences that need translation
        for i, sentence in enumerate(sentences):
            if not sentence:
                result_sentences[i] = sentence
                continue
                
            if NON_ZH_PATTERN.search(sentence):
                masked, ph_map = self.mask_protected_terms(sentence, protected_terms)
                
                if self.ratio_of_chinese(masked) < threshold:
                    to_translate.append(masked)
                    indices.append(i)
                    ph_maps.append(ph_map)
                else:
                    result_sentences[i] = sentence
            else:
                result_sentences[i] = sentence
        
        # Only call batch_translate once with all sentences
        if to_translate:
            translated_batch = self.batch_translate(to_translate)
            
            # Process results back into original positions
            for j, (idx, translated, ph_map) in enumerate(zip(indices, translated_batch, ph_maps)):
                result_sentences[idx] = self.unmask_protected_terms(translated, ph_map)
        
        # Efficiently reassemble text
        result = []
        for i, sentence in enumerate(result_sentences):
            result.append(sentence)
            if i < len(separators):
                result.append(separators[i])
        
        return ''.join(result)

    def chinese_semantic_clean(
        self,
        texts: List[str],
        freq_threshold: float = 0.5,          # è¯é¢‘é˜ˆå€¼
        doc_freq_threshold: float = 0.8,       # æ–‡æ¡£é¢‘ç‡é˜ˆå€¼
        min_word_length: int = 2,              # æœ€å°è¯é•¿åº¦ï¼ˆè¿‡æ»¤å•å­—è¯ï¼‰
        custom_stopwords: Optional[Set[str]] = None,
        domain_keywords: Optional[Set[str]] = None,
        return_noise_terms: bool = False,       # æ˜¯å¦è¿”å›è¯†åˆ«å‡ºçš„å™ªå£°è¯
        verbose: bool = True
    ) -> Union[List[str], tuple]:
        """
        åŸºäºè¯é¢‘ç»Ÿè®¡çš„ä¸­æ–‡è¯­ä¹‰æ¸…æ´—æ–‡æœ¬ä¸­çš„å™ªå£°è¯ã€‚
        é€šè¿‡å¹¶è¡Œå¤„ç†åŠ é€Ÿæ¸…æ´—è¿‡ç¨‹ï¼Œæ”¯æŒè‡ªå®šä¹‰åœç”¨è¯å’Œé¢†åŸŸå…³é”®è¯ã€‚
        """
        if not texts:
            return [] if not return_noise_terms else ([], set())
            
        # Merge default and custom stopwords more efficiently
        stopwords = {
            # æƒ…æ„Ÿå¼ºåŒ–è¯
            "çœŸçš„", "çœŸæ˜¯", "å¤ª", "å¥½", "å¾ˆ", "éå¸¸", "è¶…çº§", "ç»å¯¹", "ç®€ç›´",
            # ç½‘ç»œç”¨è¯­
            "å“ˆå“ˆ", "å“ˆå“ˆå“ˆ", "å•Šå•Š", "å•Šå•Šå•Š", "å‘œå‘œ", "å‘œå‘œå‘œ", "omg", "OMG",
            "xswl", "awsl", "yyds", "ç»ç»å­", "æ— è¯­å­",
            # å£å¤´ç¦…
            "çœŸçš„æ˜¯", "å°±æ˜¯", "åæ­£", "ç„¶å", "å…¶å®", "é‚£ä¸ª", "è¿™ä¸ª", "æ‰€ä»¥",
            "emmm", "emm", "å•Šè¿™", "è¹²ä¸€ä¸ª", "å†²é¸­",
            # æ ‡ç‚¹ç¬¦å·ç»„åˆ
            "ï½ï½", "â€¦"
        }
        
        if custom_stopwords:
            stopwords.update(custom_stopwords)
        
        # Initialize word statistics
        domain_keywords = domain_keywords or set()
        total_docs = len(texts)
        word_counts = Counter()
        doc_counts = defaultdict(int)
        
        if verbose and total_docs > 1:
            print(f"æ­£åœ¨ç»Ÿè®¡è¯é¢‘ï¼ˆå…±{total_docs}ç¯‡æ–‡æœ¬ï¼‰...")
        
        # Process all texts in one loop
        for text in texts:
            # Cut words and filter by length in one operation
            words = [w for w in jieba.lcut(text) if len(w) >= min_word_length]
            
            # Update global word frequency
            word_counts.update(words)
            
            # Update document frequency with unique words
            unique_words = set(words)
            for word in unique_words:
                doc_counts[word] += 1
        
        # Calculate relative frequencies
        total_words = sum(word_counts.values()) or 1  # Avoid division by zero
        word_freq = {word: count/total_words for word, count in word_counts.items()}
        doc_freq = {word: count/total_docs for word, count in doc_counts.items()}
        
        # Adjust thresholds for small datasets more concisely
        if total_docs < 100:
            freq_threshold = min(0.8, freq_threshold * 2)
            doc_freq_threshold = min(0.9, doc_freq_threshold * 1.5)
        
        # Identify noise terms more efficiently
        noise_terms = {
            word for word, freq in word_freq.items()
            if ((freq > freq_threshold or doc_freq[word] > doc_freq_threshold) and 
                word not in domain_keywords and len(word) <= 2)
        }
        
        # Add longer stopwords
        noise_terms.update(w for w in stopwords if len(w) >= min_word_length)
        
        # Build pattern and clean texts
        if noise_terms:
            pattern = re.compile('|'.join(map(re.escape, noise_terms)))
            cleaned_texts = [pattern.sub('', text) for text in texts]
        else:
            cleaned_texts = texts[:]
        
        # Report processing stats if necessary
        if verbose and total_docs > 1:
            total_orig_len = sum(len(t) for t in texts)
            total_cleaned_len = sum(len(t) for t in cleaned_texts)
            
            if total_orig_len > 0:
                ratio = total_cleaned_len / total_orig_len
                print(f"å·²è¯†åˆ«å™ªå£°è¯{len(noise_terms)}ä¸ªï¼Œæ¸…æ´—å®Œæˆ!")
                print(f"å™ªå£°å»é™¤ç‡: {(1-ratio):.2%}")
                print(f"åŸå§‹/æ¸…æ´—åå­—ç¬¦æ•°: {total_orig_len}/{total_cleaned_len}")
        
        return (cleaned_texts, noise_terms) if return_noise_terms else cleaned_texts
        
    def process_text(self, text: str) -> str:
        """å®Œæ•´çš„ä¸­è‹±åŒè¯­å•æ–‡æœ¬å¤„ç†æµç¨‹, ä¸æä¾›ç¿»è¯‘åŠŸèƒ½
        Args:
            text: å¾…å¤„ç†æ–‡æœ¬
        Returns:
            str: å¤„ç†åçš„æ–‡æœ¬
        å¤„ç†æµç¨‹:
            1. åŸºç¡€æ¸…æ´—(å»é™¤HTMLæ ‡ç­¾ã€URLã€ç‰¹æ®Šç¬¦å·ç­‰, å…¨è§’è½¬åŠè§’, ç©ºæ ¼æ ‡å‡†åŒ–)
            2. ç¤¾äº¤åª’ä½“ç‰¹å®šæ¸…æ´— (å»é™¤è¯é¢˜æ ‡ç­¾ã€@æåŠã€è¡¨æƒ…åŒ…çš„demojize, å¤„ç†å°çº¢ä¹¦ç‰¹å®šæ ‡ç­¾)
            3. è¯­ä¹‰æ¸…æ´— (jiebaä¸­æ–‡åˆ†è¯, æ ¹æ®TF-IDFå»é™¤å™ªå£°è¯, å¹¶ä¿æŠ¤é¢†åŸŸå…³é”®è¯ï¼‰             
        """
        # Early return for empty text
        if not text or text.strip() == '':
            return ""
        
        # Register domain keywords with jieba
        self.register_domain_keywords_with_jieba(self.domain_keywords)

        # 1. åŸºç¡€æ¸…æ´—
        text = self.basic_clean(text)

        # 2. ç¤¾äº¤åª’ä½“ç‰¹å®šæ¸…æ´—
        text = self.social_media_clean(text)

        # 3. è¯­ä¹‰æ¸…æ´— (å•æ¡å¤„ç†ä¸æä¾›ç¿»è¯‘åŠŸèƒ½)
        text = self.chinese_semantic_clean([text], verbose=False)[0]

        return text
        
    def batch_process(self, texts, enable_translation=True, verbose=False) -> List[str]:
        """
        å®Œæ•´çš„ä¸­è‹±åŒè¯­å¤šæ–‡æœ¬å¤„ç†æµç¨‹, ç¿»è¯‘é»˜è®¤å¼€å¯, CPUå’Œchunk_sizeè‡ªåŠ¨è°ƒæ•´
        åŒæ—¶, å¦‚æœæ–‡æœ¬æ•°é‡å°äº100, åˆ™ä¸ä½¿ç”¨å¹¶è¡Œå¤„ç†, å¹¶å…³é—­ç¿»è¯‘
        å¦‚æœæ–‡æœ¬æ•°é‡å¤§äº100, ä½¿ç”¨å¹¶è¡Œå¤„ç†çš„åŒæ—¶, æ‰¹é‡ç¿»è¯‘ä»¥ä¼˜åŒ–deep_translator APIè°ƒç”¨

        Args:
            texts: å¾…å¤„ç†æ–‡æœ¬åˆ—è¡¨
            enable_translation: æ˜¯å¦å¯ç”¨ç¿»è¯‘
            verbose: æ˜¯å¦æ‰“å°å¤„ç†ä¿¡æ¯

        Returns:
            List[str]: å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨
        
        å¤„ç†æµç¨‹:
            1. åŸºç¡€æ¸…æ´—(å»é™¤HTMLæ ‡ç­¾ã€URLã€ç‰¹æ®Šç¬¦å·ç­‰, å…¨è§’è½¬åŠè§’, ç©ºæ ¼æ ‡å‡†åŒ–)
            2. ç¤¾äº¤åª’ä½“ç‰¹å®šæ¸…æ´— (å»é™¤è¯é¢˜æ ‡ç­¾ã€@æåŠã€è¡¨æƒ…åŒ…çš„demojize, å¤„ç†å°çº¢ä¹¦ç‰¹å®šæ ‡ç­¾)
            3. ä¸­è‹±è¯­å¢ƒä¼˜åŒ–ï¼ˆç”¨langidåšè¯­è¨€æ£€æµ‹, å¹¶åœ¨å¥å­ä¸­æ–‡å«é‡ä½äº60%æ—¶ç”¨deep_translator APIç¿»è¯‘ï¼‰
            4. è¯­ä¹‰æ¸…æ´— (jiebaä¸­æ–‡åˆ†è¯, æ ¹æ®TF-IDFå»é™¤å™ªå£°è¯, å¹¶ä¿æŠ¤é¢†åŸŸå…³é”®è¯ï¼‰
        """
        if not texts:
            return []
    
        # ç¡®å®šæ˜¯å¦æ‰§è¡Œç¿»è¯‘ - åªæœ‰å½“enable_translation=Trueä¸”æ–‡æœ¬æ•°é‡>100æ—¶æ‰å¯ç”¨
        actual_enable_translation = enable_translation and len(texts) > 100
        
        # Register domain keywords once for all processing
        self.register_domain_keywords_with_jieba(self.domain_keywords)
        
        # For small datasets, process sequentially without translation
        if len(texts) <= 100:
            if verbose:
                print(f"Processing {len(texts)} texts sequentially (translation disabled)")
            return [self.process_text(text) for text in texts]
        
        # For larger datasets, use parallelization
        effective_cores = min(self.max_workers, os.cpu_count() or 4)
        chunk_size = max(10, len(texts) // effective_cores)
        
        if verbose:
            print(f"Processing {len(texts)} texts with {effective_cores} cores")
            print(f"Translation {'enabled' if actual_enable_translation else 'disabled'}")
        
        # Define a more efficient chunk processing function
        def process_chunk(chunk):
            # Create a pipeline for each text in the chunk
            processed_texts = []
            
            for text in chunk:
                if not text or not text.strip():
                    processed_texts.append("")
                    continue
                
                # Phase 1: Basic cleaning
                cleaned = self.basic_clean(text)
                
                # Phase 2: Social media cleaning
                cleaned = self.social_media_clean(cleaned)
                
                # Phase 3: Language optimization (only with translation enabled)
                if actual_enable_translation and NON_ZH_PATTERN.search(cleaned):
                    cleaned = self.language_optimize(
                        cleaned,
                        protected_terms=self.domain_keywords,
                        enable_translation=True
                    )
                
                processed_texts.append(cleaned)
            
            # Phase 4: Semantic cleaning (done as batch)
            return self.chinese_semantic_clean(processed_texts, verbose=False)
        
        # Process chunks in parallel
        chunks = [texts[i:i+chunk_size] for i in range(0, len(texts), chunk_size)]
        
        with ThreadPoolExecutor(max_workers=effective_cores) as executor:
            chunk_results = list(executor.map(process_chunk, chunks))
        
        # Flatten results
        return [text for chunk in chunk_results for text in chunk]


    def get_stats(self) -> dict:
        """è¿”å›å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cache_size": len(self.translation_cache),
            "cache_capacity": self.cache_size,
            "cache_hits": self.cache_hits,
            "translation_requests": self.translation_requests,
            "cache_hit_rate": self.cache_hits / max(1, self.translation_requests)
        }

# # Define functions for text processing used in the class
# DOMAIN_KEYWORDS = {'é²œèŠ‹ä»™', 'Meet Fresh', 'MeetFresh', 'å°æ¹¾ç¾é£Ÿ', 'ç”œå“', 
#         'èŠ‹åœ†', 'taro ball', 'èŠ‹å¤´', 'taro', 'ä»™è‰', 'grass jelly', 'å¥¶èŒ¶', 'milk tea',
#         'è±†èŠ±', 'tofu pudding', 'å¥¶åˆ¨å†°', 'milked shaved ice',
#         'çº¢è±†æ±¤', 'purple rice soup', 'ç´«ç±³ç²¥', 'red bean soup',
#         '2001 Coit Rd', 'Park Pavillion Center', '(972) 596-6088',
#         'é¤å…', 'é¤é¦†', 'ç¾é£Ÿ', 'å°æ¹¾å°åƒ', 'å°æ¹¾ç”œå“', 'å†°æ¿€å‡Œ',
#         "VIP", "AI", "DFW", "Dallas", "è¾¾æ‹‰æ–¯"}