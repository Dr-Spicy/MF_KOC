{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf04df9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b41847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from content_cleaning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f23be91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批量处理耗时: 449.05秒\n"
     ]
    }
   ],
   "source": [
    "# 初始化处理器\n",
    "processor = XHSMixedLanguageProcessor(cache_size=2000, max_workers=12)\n",
    "\n",
    "# # 示例笔记\n",
    "# sample_notes = [\n",
    "#     \"今天去了Dallas的鲜芋仙 #美食打卡# 这家MeetFresh真的超级好吃！The taro balls were amazing! 强烈推荐大家尝试～\",\n",
    "#     \"新开的台湾甜品店@小红书美食博主 服务态度nice，芋圆Q弹，仙草冻很香 #DFW美食# [笑哭R] 2001 Coit Rd真的很方便\",\n",
    "#     \"约上闺蜜一起去吃甜品，牛奶冰+芋圆组合👍 The dessert was incredibly refreshing on such a hot day! https://xiaohongshu.com/...\",\n",
    "#     \"今日份打卡：鲜芋仙 Park Pavillion Center，人均$15左右，店内环境整洁，服务态度很好，definitely worth the price!\",\n",
    "# ]\n",
    "\n",
    "# processed_text = processor.process_text(sample_notes[0], enable_translation=True)\n",
    "# print(\"单条处理结果:\", processed_text)\n",
    "# processed_texts = processor.batch_process(sample_notes)\n",
    "# print(\"\\n批量处理结果:\")\n",
    "# for i, text in enumerate(processed_texts):\n",
    "#     print(f\"{i+1}. {text}\")\n",
    "\n",
    "# Load the cooked contents data\n",
    "cont = pd.read_json('..\\..\\Data\\processed\\contents_cooked.json')\n",
    "\n",
    "# combine the title and note_body into a single string\n",
    "def process_text(note):\n",
    "    return note['title'] + ' ' + note['note_body']\n",
    "\n",
    "# Apply the function to the DF\n",
    "cont['text'] = cont.apply(process_text, axis=1).astype(str)\n",
    "\n",
    "# Apply the batch processing function to the DF.text column\n",
    "start_time = time.time()\n",
    "processed_texts = processor.batch_process(cont['text'].tolist(), enable_translation=True)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"批量处理耗时: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "# save the processed texts to a new column in the DF\n",
    "cont['semantic_proc_text'] = processed_texts\n",
    "# remove the original text column\n",
    "cont.drop(columns=['text'], inplace=True)\n",
    "# save the processed DF to a new JSON file\n",
    "cont.to_json('..\\..\\Data\\processed\\contents_cooked_semantic.json', orient='records', lines=True, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d858a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['note_id', 'user_id', 'title', 'note_body', 'tag_list', 'image_count',\n",
       "       'content_type_video', 'hot_note', 'post_time', 'last_update_time',\n",
       "       'scraped_time', 'elapsed_time', 'liked_count', 'collected_count',\n",
       "       'comment_count', 'share_count', 'interaction_count',\n",
       "       'semantic_proc_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d240b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_note = cont.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9031c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = processor.batch_process(cont_sample.text.tolist(), verbose=True)\n",
    "print(\"\\n批量处理结果:\")\n",
    "for i, text in enumerate(processed_texts):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61cc7913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cache_size': 61,\n",
       " 'cache_capacity': 2000,\n",
       " 'cache_hits': 2,\n",
       " 'translation_requests': 61,\n",
       " 'cache_hit_rate': 0.03278688524590164}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8cddd",
   "metadata": {},
   "source": [
    "定义domain_keywords, 并提前compile 正则表达式和实例化重复使用的翻译器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddcaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add domain keywords\n",
    "DOMAIN_KEYWORDS = {\n",
    "    '鲜芋仙', 'Meet Fresh', 'MeetFresh', '台湾美食', '甜品', \n",
    "    '芋圆', 'taro', '仙草', 'grass jelly', '奶茶', 'milk tea',\n",
    "    '豆花', 'tofu pudding', '奶刨冰', 'milked shaved ice',\n",
    "    '红豆汤', 'purple rice soup', '紫米粥', 'red bean soup',\n",
    "    '2001 Coit Rd', 'Park Pavillion Center', '(972) 596-6088',\n",
    "    '餐厅', '餐馆', '美食', '台湾小吃', '台湾甜品', '冰激凌'\n",
    "}\n",
    "\n",
    "# Pre-compile regex patterns for efficiency\n",
    "ZH_CHAR_PATTERN = re.compile(r'[\\u4e00-\\u9fff]')  # 中文字符检测\n",
    "NON_ZH_PATTERN = re.compile(r'[a-zA-Z]')  # 拉丁字母检测\n",
    "SPLIT_PATTERN = re.compile(r'([。！？?!.])')  # 分句正则\n",
    "URL_PATTERN = re.compile(\n",
    "    r'(?:https?://)?(?:[a-zA-Z0-9\\u4e00-\\u9fff-]+\\.)+[a-zA-Z]{2,}'\n",
    "    r'(?:/\\S*)?(?:\\?\\S*)?(?:#\\S*)?',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "TOPIC_PATTERN = re.compile(r'#([^#]+)#')\n",
    "MENTION_PATTERN = re.compile(r'@[\\w\\u4e00-\\u9fff-]+')  # 支持中英文用户名\n",
    "XIAOHONGSHU_TAG_PATTERN = re.compile(r'\\[(话题|表情|地点)\\s*[^\\]]*\\]')\n",
    "BRACKET_CONTENT_PATTERN = re.compile(r'\\[([^\\]]+)\\]')\n",
    "HTML_TAG_PATTERN = re.compile(r'<[^>]+>')\n",
    "WHITESPACE_PATTERN = re.compile(r'[\\t\\n\\u3000]')\n",
    "MULTI_SPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "# Translation cache\n",
    "translation_cache = {}\n",
    "\n",
    "# Initialize translator once\n",
    "_translator = GoogleTranslator(source='auto', target='zh-CN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77036b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSanitizer:\n",
    "    def __init__(self):\n",
    "        # 预编译所有正则表达式\n",
    "        self.mention_pattern = re.compile(r'@\\w+') # @提及正则\n",
    "        self.url_pattern = re.compile(r'(https?://\\S+)') # URL正则\n",
    "        self.emoji_dict = emoji.EMOJI_DATA  # 预加载表情符号库\n",
    "        self.protected_terms = self._load_protected_terms()\n",
    "        \n",
    "    def _load_protected_terms(self):\n",
    "        # 从文件/数据库加载保护词表（品牌词、产品词等）\n",
    "        return {'鲜芋仙', 'MeetFresh', 'ParkPavilion'}  \n",
    "    \n",
    "    def pipeline(self, text):\n",
    "        \"\"\"单条文本处理管道\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        text = basic_clean(text)\n",
    "        text = social_media_clean(text)\n",
    "        text = language_optimize(text, protected_terms=self.protected_terms, enable_translation=True)\n",
    "        return text\n",
    "    \n",
    "    def batch_process(self, texts):\n",
    "        \"\"\"批量处理文本\"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "            \n",
    "        # 计算合适的核心数\n",
    "        num_cores = max(1, os.cpu_count() * 8 // 10)\n",
    "        chunk_size = max(1, len(texts) // (num_cores * 4))\n",
    "        \n",
    "        # 合并相似处理步骤，减少线程切换\n",
    "        def process_chunk(chunk):\n",
    "            return [self.pipeline(text) for text in chunk]\n",
    "        \n",
    "        # 创建文本块\n",
    "        chunks = [texts[i:i+chunk_size] for i in range(0, len(texts), chunk_size)]\n",
    "        \n",
    "        # 并行处理\n",
    "        with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "            results = list(executor.map(process_chunk, chunks))\n",
    "        \n",
    "        # 展平结果\n",
    "        processed_texts = []\n",
    "        for chunk_result in results:\n",
    "            processed_texts.extend(chunk_result)\n",
    "        \n",
    "        return processed_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b0c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb67ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cooked contents data\n",
    "cont = pd.read_json('..\\..\\Data\\processed\\contents_cooked.json')\n",
    "\n",
    "# combine the title and note_body into a single string\n",
    "def process_text(note):\n",
    "    return note['title'] + ' ' + note['note_body']\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "cont['text'] = cont.apply(process_text, axis=1).astype(str)\n",
    "\n",
    "# Load the MeetFresh user dictionary\n",
    "jieba.load_userdict(\"MF_dict.txt\")\n",
    "\n",
    "# prepare the regex patterns for text processing\n",
    "ZH_CHAR_PATTERN = re.compile(r'[\\u4e00-\\u9fff]')  # 中文字符检测\n",
    "NON_ZH_PATTERN = re.compile(r'[a-zA-Z]')  # 拉丁字母检测\n",
    "SPLIT_PATTERN = re.compile(r'([。！？?!.])')  # 分句正则\n",
    "\n",
    "# 复用翻译器实例（降低初始化开销）\n",
    "_translator = GoogleTranslator(source='auto', target='zh-CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_keywords = {\n",
    "    '鲜芋仙', 'Meet Fresh', 'MeetFresh', '台湾美食', '甜品', \n",
    "    '芋圆', 'taro', '仙草', 'grass jelly', '奶茶', 'milk tea',\n",
    "    '豆花', 'tofu pudding', '奶刨冰', 'milked shaved ice',\n",
    "    '红豆汤', 'purple rice soup', '紫米粥', 'red bean soup',\n",
    "    '2001 Coit Rd', 'Park Pavillion Center', '(972) 596-6088',\n",
    "    '餐厅', '餐馆', '美食', '台湾小吃', '台湾甜品', '冰激凌'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the 1st row of cont\n",
    "cont.iloc[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7090e",
   "metadata": {},
   "source": [
    "中文文本清洗黄金四步法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731d2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullwidth_to_halfwidth(text:str) -> str:\n",
    "    \"\"\"全角转半角（保留￥符号）\"\"\"\n",
    "    translation_table = str.maketrans({\n",
    "        '！': '!', '“': '\"', '”': '\"', '‘': \"'\", '’': \"'\",\n",
    "        '、': ',', '，': ',', '；': ';', '：': ':', '？': '?',\n",
    "        '《': '<', '》': '>', '【': '[', '】': ']', '·': '.',\n",
    "        '～': '~', '—': '-', '（': '(', '）': ')', '　': ' '\n",
    "    })\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "def normalize_punctuation(text:str) -> str:\n",
    "    \"\"\"符号标准化（保留emoji, retain 字符的位置信息但牺牲了效率, 之后可以考虑优化\"\"\"\n",
    "    # 定义保留符号集（新增%$￥）\n",
    "    keep_symbols = {\"'\", '\"', ',', '.', ':', ';', '!', '?', '-', \n",
    "                   '(', ')', '<', '>', '[', ']', '&', '#', '@',\n",
    "                   '%', '$', '￥', '/', '=', '+', '~', '^'}\n",
    "    \n",
    "    # 字符级处理, \n",
    "    cleaned_chars = []\n",
    "    for char in text:\n",
    "        # 保留条件：字母数字/汉字/keep_symbols/emoji\n",
    "        if (char.isalnum() or\n",
    "            '\\u4e00' <= char <= '\\u9fff' or\n",
    "            char in keep_symbols or\n",
    "            emoji.is_emoji(char)):\n",
    "            cleaned_chars.append(char)\n",
    "        else:\n",
    "            cleaned_chars.append(' ')\n",
    "    \n",
    "    return ''.join(cleaned_chars)\n",
    "\n",
    "def remove_urls(text:str) -> str:\n",
    "    \"\"\"适配中文域名的URL移除\"\"\"\n",
    "    url_pattern = re.compile(\n",
    "        r'(?:https?://)?(?:[a-zA-Z0-9\\u4e00-\\u9fff-]+\\.)+[a-zA-Z]{2,}'\n",
    "        r'(?:/\\S*)?(?:\\?\\S*)?(?:#\\S*)?',\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "def basic_clean(\n",
    "        text : str\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    对文本进行基础层清洗，包括去除HTML标签、URL、特殊符号处理, 空格标准化等。\n",
    "    1. 移除HTML标签\n",
    "    2. 移除URL（适配中文域名）\n",
    "    3. 处理特殊符号（保留常用符号和emoji, 全角标点转半角）\n",
    "    4. 标准化空白字符\n",
    "\n",
    "    Args:\n",
    "        text (str): 待清洗的文本。\n",
    "    Returns:\n",
    "        str: 清洗后的文本。\n",
    "    \"\"\"\n",
    "    # 替换所有空白符（含小红书常见的全角空格\\u3000、换行、制表符）\n",
    "    text = re.sub(r'[\\t\\n\\u3000]', ' ', text)\n",
    "    # HTML标签移除\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # URL移除, 适配中文域名\n",
    "    text = remove_urls(text)\n",
    "    # 全角转半角（保留全角￥）\n",
    "    text = fullwidth_to_halfwidth(text)\n",
    "    # 符号标准化\n",
    "    text = normalize_punctuation(text)\n",
    "    # 标准化合并连续空格\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587d3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the basic_clean function to the 'text' column\n",
    "cont['text'] = cont['text'].apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd2225c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[达拉斯.吃]快乐小羊,回到儿时澳门豆捞坊 Happy Lamb Hot Pot 📍 240 Legacy Dr ste 116, Plano, TX 75023 感谢快乐小羊的邀请 虽然不知道小羊快不快乐[笑哭R]但我吃的很快乐 特别喜欢小羊家环境,舒适低调不喧哗,适合朋友聊天 所有菜品自取,更自由快捷,绝大多数都很新鲜(只有个别鹌鹑蛋黄不知为何有点咸鸭蛋味道,也许是我敏感[害羞R])我们选的金汤酸辣锅和特制香辣锅 金汤锅其实更像酸菜锅,酸菜味挺浓,还有很麻的口感,下了鱼片和牛肉,秒变酸菜鱼和酸菜牛 香辣锅是台湾健康感麻辣小火锅味儿,相对重庆火锅更为清淡,也更能体现食材本身的味道 强烈推荐他家臻品羊肉片,肥瘦合适还有奶香味 然后手打羊肉丸,和香菜混合制作,特别好吃 台湾酸甜口包心菜泡菜永远吃不腻 作为碳水大户,炒饭炒面炸馒头炸麻球完全满足了我的需求,炸鸡翅和橙子也很不错,小朋友饭应有尽有~ 重点来了!小羊给的两杯鸡尾酒把不太喝酒的我给惊艳了!图10中矮的那杯清冽的酒精带着丝丝甜味,醇香口感干脆利落,会一直想喝不停 有柠檬片的那杯是一种混合果汁配着清淡酸奶泡的无酒精鸡尾酒,打败我爱的所有果汁 没想到小羊要靠鸡尾酒出道了[笑哭R] 一顿饭下来,不仅吃的满足,整个氛围也让人想起童年回忆里的澳门豆捞坊,干净惬意有情调 我就喜欢这样安安静静享受火锅的快乐 #达拉斯火锅[话题]# #达拉斯美食[话题]# #达拉斯生活[话题]# #达拉斯[话题]# #达拉斯探店[话题]# #达拉斯周边[话题]# #达拉斯周末[话题]# #达拉斯吃喝玩乐[话题]# #快乐小羊[话题]# #快乐小羊火锅[话题]#'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont['text'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d81f2",
   "metadata": {},
   "source": [
    "社交媒体特征处理层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b77ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_media_clean(text:str, strategy='demojize') -> str:\n",
    "    \"\"\"\n",
    "    社交媒体文本清洗，主要针对小红书平台的特定格式和符号进行处理。\n",
    "    1. 移除话题标签（#）但保留关键词\n",
    "    2. 移除@提及, support 中英文复合用户名\n",
    "    3. 转换表情符号（可选：移除或转换为文本描述）\n",
    "    4. 处理小红书特有方括号\n",
    "    5. 去除多余空格\n",
    "\n",
    "    Args:\n",
    "        text (str): 待清洗的文本。\n",
    "    Returns:\n",
    "        str: 清洗后的文本。\n",
    "    \"\"\"\n",
    "\n",
    "    # 移除话题标签但保留关键词（如 #达拉斯美食# → 达拉斯美食）\n",
    "    text = re.sub(r'#([^#]+)#', r'\\1', text)\n",
    "    \n",
    "    # 移除@提及(包含其变体, 如@小红书用户)\n",
    "    text = re.sub(r'@[\\w\\u4e00-\\u9fff-]+', '', text)  # 支持中英文用户名\n",
    "    \n",
    "    # 转换Emoji（可选策略）\n",
    "    if strategy == 'remove':\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "    elif strategy == 'demojize':\n",
    "        # 将emoji转换为文本描述（如:😀 → :grinning_face:）\n",
    "        text = emoji.demojize(text, delimiters=(' [', '] '))\n",
    "    \n",
    "    # 处理小红书特有的方括号标签（删除系统标签, 如[话题]→'')\n",
    "    text = re.sub(r'\\[话题\\s*[^\\]]*\\]', '', text)  # 删除系统标签 \n",
    "\n",
    "    # 继续处理小红书方括号标签（同时保留关键文本信息, 如[笑哭R]→笑哭R）\n",
    "    text = re.sub(r'\\[(?:表情|地点)\\s*[^\\]]*\\]', '', text)\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]', r'\\1', text)  # 去除方括号但保留内容\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10bd075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the social_media_clean function to the 'text' column\n",
    "cont['text'] = cont['text'].apply(social_media_clean, strategy='demojize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cdd92",
   "metadata": {},
   "source": [
    "中文英文混合语言环境优化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b1efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_of_chinese(text: str) -> float:\n",
    "    \"\"\"返回 text 中（Unicode范围4E00-9FFF）汉字的占比。\"\"\"\n",
    "    if not text:\n",
    "        return 1.0\n",
    "    zh_chars = ZH_CHAR_PATTERN.findall(text)\n",
    "    return len(zh_chars) / len(text) if len(zh_chars) > 0 else 0.0\n",
    "\n",
    "def mask_protected_terms(text: str, protected_terms: set) -> (str, dict):\n",
    "    \"\"\"\n",
    "    将 text 中出现的保护词用占位符替换，并返回替换后的文本以及占位符映射字典。\n",
    "    比如 \"MeetFresh\" -> \"[EN_TERM_0]\"\n",
    "    \"\"\"\n",
    "    if not protected_terms:\n",
    "        return text, {}\n",
    "    \n",
    "    # 按术语长度降序排列（优先匹配长词）\n",
    "    sorted_terms = sorted(protected_terms, key=lambda x: len(x), reverse=True)\n",
    "    pattern = re.compile(\n",
    "        r'\\b(' + '|'.join(map(re.escape, sorted_terms)) + r')\\b', \n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    placeholder_map = {}\n",
    "    idx = 0\n",
    "    \n",
    "    def _repl(m):\n",
    "        nonlocal idx\n",
    "        placeholder = f\"[EN_TERM_{idx}]\"\n",
    "        placeholder_map[placeholder] = m.group(0)\n",
    "        idx += 1\n",
    "        return placeholder\n",
    "    \n",
    "    return pattern.sub(_repl, text), placeholder_map\n",
    "\n",
    "def unmask_protected_terms(text: str, placeholder_map: dict) -> str:\n",
    "    \"\"\"将翻译后文本中的占位符恢复成原始英文术语\"\"\"\n",
    "    for placeholder, original in placeholder_map.items():\n",
    "        text = text.replace(placeholder, original)\n",
    "    return text\n",
    "\n",
    "@lru_cache(maxsize=5010)\n",
    "def cached_translate(text: str) -> str:\n",
    "    \"\"\"带缓存的翻译方法，整合了安全检查\"\"\"\n",
    "    # 1. 空字符串或仅包含空格、标点、数字等，可直接返回原文\n",
    "    if not text or text.strip().isdigit():\n",
    "        return text \n",
    "    \n",
    "    # 2. 避免超过 5000 字符的文本\n",
    "    if len(text) > 5000:\n",
    "        print(f\"Warning: text too long ({len(text)} chars). Truncating for translation.\")\n",
    "        text = text[:4900]  # 适当截断以防API限制\n",
    "\n",
    "    # 3. 调用翻译，并捕捉异常\n",
    "    try:\n",
    "        return _translator.translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation failed: {e}\")\n",
    "        return text\n",
    "\n",
    "def language_optimize(\n",
    "    text: str,\n",
    "    threshold: float = 0.5,\n",
    "    protected_terms: set = None,\n",
    "    enable_translation: bool = True\n",
    ") -> str:\n",
    "    \"\"\"终极优化版语言处理\"\"\"\n",
    "    if not enable_translation or not text:\n",
    "        return text\n",
    "    \n",
    "    if protected_terms is None:\n",
    "        protected_terms = {\n",
    "            \"MeetFresh\", \"VIP\", \"AI\", \"DFW\", \n",
    "            \"Grass Jelly\", \"Taro\", \"Milk Tea\",\n",
    "            \"Red Bean Soup\", \"Purple Rice Soup\",\n",
    "            \"Tofu Pudding\", \"Shaved Ice\",\n",
    "            \"Purple Rice\", '2001 Coit Rd', 'Park Pavillion Center'\n",
    "        }\n",
    "    \n",
    "    # 预处理：快速过滤纯中文文本或空文本\n",
    "    if not text or not NON_ZH_PATTERN.search(text):\n",
    "        return text\n",
    "    \n",
    "    # 分句优化（减少内存占用）\n",
    "    buffer = []\n",
    "    segments = SPLIT_PATTERN.split(text)\n",
    "    \n",
    "    for i in range(0, len(segments), 2):\n",
    "        sentence = segments[i]\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # 快速跳过纯中文句\n",
    "        if NON_ZH_PATTERN.search(sentence):\n",
    "            # 先保护重要术语\n",
    "            masked, ph_map = mask_protected_terms(sentence, protected_terms)\n",
    "            \n",
    "            # 只翻译中文占比低于阈值的句子\n",
    "            if ratio_of_chinese(masked) < threshold:\n",
    "                translated = cached_translate(masked)\n",
    "                unmasked = unmask_protected_terms(translated, ph_map)\n",
    "                buffer.append(unmasked)\n",
    "            else:\n",
    "                buffer.append(sentence)\n",
    "        else:\n",
    "            buffer.append(sentence)\n",
    "        \n",
    "        # 添加分隔符\n",
    "        if i+1 < len(segments):\n",
    "            buffer.append(segments[i+1])\n",
    "    \n",
    "    return ''.join(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b5a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the language_optimize function to the 'text' column\n",
    "cont['text'] = cont['text'].apply(language_optimize, enable_translation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737d34d",
   "metadata": {},
   "source": [
    "高级语义清洗层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1d5c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_semantic_clean(\n",
    "    texts: List[str],\n",
    "    freq_threshold: float = 0.25,          # 词频阈值\n",
    "    doc_freq_threshold: float = 0.6,       # 文档频率阈值\n",
    "    min_word_length: int = 2,              # 最小词长度（过滤单字词）\n",
    "    custom_stopwords: Optional[Set[str]] = None,\n",
    "    domain_keywords: Optional[Set[str]] = None,\n",
    "    return_noise_terms: bool = False       # 是否返回识别出的噪声词\n",
    ") -> Union[List[str], tuple]:\n",
    "    \"\"\"\n",
    "    基于词频统计的中文语义清洗函数\n",
    "    \n",
    "    Args:\n",
    "        texts: 待清洗的文本列表\n",
    "        freq_threshold: 词频阈值，超过此阈值的词被视为噪声（除非在domain_keywords中）\n",
    "        doc_freq_threshold: 文档频率阈值，出现在超过此比例文档中的词被视为噪声\n",
    "        min_word_length: 最小词长度，小于此长度的词不参与统计\n",
    "        custom_stopwords: 自定义停用词集合\n",
    "        domain_keywords: 领域关键词集合（这些词不会被过滤）\n",
    "        return_noise_terms: 是否返回识别出的噪声词\n",
    "        \n",
    "    Returns:\n",
    "        清洗后的文本列表，如果return_noise_terms为True，则同时返回识别的噪声词集合\n",
    "    \"\"\"\n",
    "    # 默认小红书常见噪声词\n",
    "    default_stopwords = {\n",
    "        # 情感强化词\n",
    "        \"真的\", \"真是\", \"太\", \"好\", \"很\", \"非常\", \"超级\", \"绝对\", \"简直\",\n",
    "        # 网络用语\n",
    "        \"哈哈\", \"哈哈哈\", \"啊啊\", \"啊啊啊\", \"呜呜\", \"呜呜呜\", \"omg\", \"OMG\",\n",
    "        \"xswl\", \"awsl\", \"yyds\", \"绝绝子\", \"无语子\",\n",
    "        # 口头禅\n",
    "        \"真的是\", \"就是\", \"反正\", \"然后\", \"其实\", \"那个\", \"这个\", \"所以\",\n",
    "        \"emmm\", \"emm\", \"啊这\", \"蹲一个\", \"冲鸭\",\n",
    "        # 标点符号组合\n",
    "        \"～～\", \"…\"\n",
    "    }\n",
    "    \n",
    "    # 合并自定义停用词\n",
    "    stopwords = default_stopwords.copy()\n",
    "    if custom_stopwords:\n",
    "        stopwords.update(custom_stopwords)\n",
    "    \n",
    "    # 初始化领域关键词集合\n",
    "    domain_keywords = domain_keywords or set()\n",
    "    \n",
    "    # 统计词频和文档频率\n",
    "    total_docs = len(texts)\n",
    "    word_counts = Counter()\n",
    "    doc_counts = defaultdict(int)\n",
    "    \n",
    "    print(f\"正在统计词频（共{total_docs}篇文本）...\")\n",
    "    for text in texts:\n",
    "        words = jieba.lcut(text)\n",
    "        # 过滤太短的词\n",
    "        words = [w for w in words if len(w) >= min_word_length]\n",
    "        \n",
    "        # 更新全局词频\n",
    "        word_counts.update(words)\n",
    "        \n",
    "        # 更新文档频率（每个文档中的词只计算一次）\n",
    "        unique_words = set(words)\n",
    "        for word in unique_words:\n",
    "            doc_counts[word] += 1\n",
    "    \n",
    "    # 计算相对频率\n",
    "    word_freq = {word: count/total_docs for word, count in word_counts.items()}\n",
    "    doc_freq = {word: count/total_docs for word, count in doc_counts.items()}\n",
    "    \n",
    "    # 识别噪声词（高频但不是领域关键词）\n",
    "    noise_terms = {\n",
    "        word for word, freq in word_freq.items()\n",
    "        if (freq > freq_threshold or  # 全局高频\n",
    "            doc_freq[word] > doc_freq_threshold) and  # 文档高频\n",
    "           word not in domain_keywords  # 不是领域关键词\n",
    "    }\n",
    "    \n",
    "    # 合并自定义停用词\n",
    "    noise_terms.update(stopwords)\n",
    "    \n",
    "    print(f\"已识别噪声词{len(noise_terms)}个，开始清洗...\")\n",
    "    \n",
    "    # 构建过滤模式\n",
    "    pattern = re.compile('|'.join(noise_terms))\n",
    "    cleaned_texts = [pattern.sub('', text) for text in texts]\n",
    "    \n",
    "    # 评估整体效果\n",
    "    total_original_length = sum(len(t) for t in texts)\n",
    "    total_cleaned_length = sum(len(t) for t in cleaned_texts)\n",
    "    compression_ratio = total_cleaned_length / total_original_length\n",
    "    \n",
    "    print(f\"清洗完成! 噪声去除率: {(1-compression_ratio):.2%}\")\n",
    "    print(f\"原始总字符数: {total_original_length}\")\n",
    "    print(f\"清洗后总字符数: {total_cleaned_length}\")\n",
    "    \n",
    "    if return_noise_terms:\n",
    "        return cleaned_texts, noise_terms\n",
    "    \n",
    "    return cleaned_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63edad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the chinese_semantic_clean function to the 'text' column\n",
    "cont['text'] = chinese_semantic_clean(\n",
    "    cont['text'].tolist(),\n",
    "    freq_threshold=0.25,\n",
    "    doc_freq_threshold=0.6,\n",
    "    min_word_length=2,\n",
    "    custom_stopwords=None,\n",
    "    domain_keywords=domain_keywords,\n",
    "    return_noise_terms=False\n",
    ")\n",
    "# 语义清洗后，去除空文本\n",
    "cont = cont[cont['text'].str.strip() != '']\n",
    "#cont = cont.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175e156",
   "metadata": {},
   "source": [
    "全流程集成示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4609cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSanitizer:\n",
    "    def __init__(self):\n",
    "        self.mention_pattern = re.compile(r'@\\w+') # @提及正则\n",
    "        self.url_pattern = re.compile(r'(https?://\\S+)') # URL正则\n",
    "        self.emoji_dict = emoji.EMOJI_DATA  # 预加载表情符号库\n",
    "        self.protected_terms = self._load_protected_terms()\n",
    "        \n",
    "    def _load_protected_terms(self):\n",
    "        # 从文件/数据库加载保护词表（品牌词、产品词等）\n",
    "        return {'鲜芋仙', 'MeetFresh', 'ParkPavilion'}  \n",
    "    \n",
    "    def pipeline(self, text):\n",
    "        text = basic_clean(text)\n",
    "        text = social_media_clean(text)\n",
    "        text = language_optimize(text, protected_terms=self.protected_terms, enable_translation=True)\n",
    "        text = language_optimize(text)\n",
    "        return text\n",
    "    \n",
    "    def batch_process(self, texts):\n",
    "        # 并行加速（利用80%CPU核心）\n",
    "        num_cores = os.cpu_count() * 8 // 10\n",
    "        # 确保至少使用一个核心\n",
    "        num_cores = max(1, num_cores)\n",
    "        # 使用ProcessPoolExecutor进行并行处理\n",
    "        with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "            return list(executor.map(self.pipeline, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35937f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化 TextSanitizer 类\n",
    "sanitizer = TextSanitizer()\n",
    "# 批量处理文本\n",
    "cont['text'] = sanitizer.batch_process(cont['text'].tolist())\n",
    "# 语义清洗后，去除空文本\n",
    "cont = cont[cont['text'].str.strip() != '']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
