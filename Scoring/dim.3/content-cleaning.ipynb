{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf04df9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b41847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from content_cleaning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f23be91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰¹é‡å¤„ç†è€—æ—¶: 449.05ç§’\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–å¤„ç†å™¨\n",
    "processor = XHSMixedLanguageProcessor(cache_size=2000, max_workers=12)\n",
    "\n",
    "# # ç¤ºä¾‹ç¬”è®°\n",
    "# sample_notes = [\n",
    "#     \"ä»Šå¤©å»äº†Dallasçš„é²œèŠ‹ä»™ #ç¾é£Ÿæ‰“å¡# è¿™å®¶MeetFreshçœŸçš„è¶…çº§å¥½åƒï¼The taro balls were amazing! å¼ºçƒˆæ¨èå¤§å®¶å°è¯•ï½\",\n",
    "#     \"æ–°å¼€çš„å°æ¹¾ç”œå“åº—@å°çº¢ä¹¦ç¾é£Ÿåšä¸» æœåŠ¡æ€åº¦niceï¼ŒèŠ‹åœ†Qå¼¹ï¼Œä»™è‰å†»å¾ˆé¦™ #DFWç¾é£Ÿ# [ç¬‘å“­R] 2001 Coit RdçœŸçš„å¾ˆæ–¹ä¾¿\",\n",
    "#     \"çº¦ä¸Šé—ºèœœä¸€èµ·å»åƒç”œå“ï¼Œç‰›å¥¶å†°+èŠ‹åœ†ç»„åˆğŸ‘ The dessert was incredibly refreshing on such a hot day! https://xiaohongshu.com/...\",\n",
    "#     \"ä»Šæ—¥ä»½æ‰“å¡ï¼šé²œèŠ‹ä»™ Park Pavillion Centerï¼Œäººå‡$15å·¦å³ï¼Œåº—å†…ç¯å¢ƒæ•´æ´ï¼ŒæœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œdefinitely worth the price!\",\n",
    "# ]\n",
    "\n",
    "# processed_text = processor.process_text(sample_notes[0], enable_translation=True)\n",
    "# print(\"å•æ¡å¤„ç†ç»“æœ:\", processed_text)\n",
    "# processed_texts = processor.batch_process(sample_notes)\n",
    "# print(\"\\næ‰¹é‡å¤„ç†ç»“æœ:\")\n",
    "# for i, text in enumerate(processed_texts):\n",
    "#     print(f\"{i+1}. {text}\")\n",
    "\n",
    "# Load the cooked contents data\n",
    "cont = pd.read_json('..\\..\\Data\\processed\\contents_cooked.json')\n",
    "\n",
    "# combine the title and note_body into a single string\n",
    "def process_text(note):\n",
    "    return note['title'] + ' ' + note['note_body']\n",
    "\n",
    "# Apply the function to the DF\n",
    "cont['text'] = cont.apply(process_text, axis=1).astype(str)\n",
    "\n",
    "# Apply the batch processing function to the DF.text column\n",
    "start_time = time.time()\n",
    "processed_texts = processor.batch_process(cont['text'].tolist(), enable_translation=True)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"æ‰¹é‡å¤„ç†è€—æ—¶: {end_time - start_time:.2f}ç§’\")\n",
    "\n",
    "# save the processed texts to a new column in the DF\n",
    "cont['semantic_proc_text'] = processed_texts\n",
    "# remove the original text column\n",
    "cont.drop(columns=['text'], inplace=True)\n",
    "# save the processed DF to a new JSON file\n",
    "cont.to_json('..\\..\\Data\\processed\\contents_cooked_semantic.json', orient='records', lines=True, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d858a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['note_id', 'user_id', 'title', 'note_body', 'tag_list', 'image_count',\n",
       "       'content_type_video', 'hot_note', 'post_time', 'last_update_time',\n",
       "       'scraped_time', 'elapsed_time', 'liked_count', 'collected_count',\n",
       "       'comment_count', 'share_count', 'interaction_count',\n",
       "       'semantic_proc_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d240b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_note = cont.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9031c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = processor.batch_process(cont_sample.text.tolist(), verbose=True)\n",
    "print(\"\\næ‰¹é‡å¤„ç†ç»“æœ:\")\n",
    "for i, text in enumerate(processed_texts):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61cc7913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cache_size': 61,\n",
       " 'cache_capacity': 2000,\n",
       " 'cache_hits': 2,\n",
       " 'translation_requests': 61,\n",
       " 'cache_hit_rate': 0.03278688524590164}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8cddd",
   "metadata": {},
   "source": [
    "å®šä¹‰domain_keywords, å¹¶æå‰compile æ­£åˆ™è¡¨è¾¾å¼å’Œå®ä¾‹åŒ–é‡å¤ä½¿ç”¨çš„ç¿»è¯‘å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddcaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add domain keywords\n",
    "DOMAIN_KEYWORDS = {\n",
    "    'é²œèŠ‹ä»™', 'Meet Fresh', 'MeetFresh', 'å°æ¹¾ç¾é£Ÿ', 'ç”œå“', \n",
    "    'èŠ‹åœ†', 'taro', 'ä»™è‰', 'grass jelly', 'å¥¶èŒ¶', 'milk tea',\n",
    "    'è±†èŠ±', 'tofu pudding', 'å¥¶åˆ¨å†°', 'milked shaved ice',\n",
    "    'çº¢è±†æ±¤', 'purple rice soup', 'ç´«ç±³ç²¥', 'red bean soup',\n",
    "    '2001 Coit Rd', 'Park Pavillion Center', '(972) 596-6088',\n",
    "    'é¤å…', 'é¤é¦†', 'ç¾é£Ÿ', 'å°æ¹¾å°åƒ', 'å°æ¹¾ç”œå“', 'å†°æ¿€å‡Œ'\n",
    "}\n",
    "\n",
    "# Pre-compile regex patterns for efficiency\n",
    "ZH_CHAR_PATTERN = re.compile(r'[\\u4e00-\\u9fff]')  # ä¸­æ–‡å­—ç¬¦æ£€æµ‹\n",
    "NON_ZH_PATTERN = re.compile(r'[a-zA-Z]')  # æ‹‰ä¸å­—æ¯æ£€æµ‹\n",
    "SPLIT_PATTERN = re.compile(r'([ã€‚ï¼ï¼Ÿ?!.])')  # åˆ†å¥æ­£åˆ™\n",
    "URL_PATTERN = re.compile(\n",
    "    r'(?:https?://)?(?:[a-zA-Z0-9\\u4e00-\\u9fff-]+\\.)+[a-zA-Z]{2,}'\n",
    "    r'(?:/\\S*)?(?:\\?\\S*)?(?:#\\S*)?',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "TOPIC_PATTERN = re.compile(r'#([^#]+)#')\n",
    "MENTION_PATTERN = re.compile(r'@[\\w\\u4e00-\\u9fff-]+')  # æ”¯æŒä¸­è‹±æ–‡ç”¨æˆ·å\n",
    "XIAOHONGSHU_TAG_PATTERN = re.compile(r'\\[(è¯é¢˜|è¡¨æƒ…|åœ°ç‚¹)\\s*[^\\]]*\\]')\n",
    "BRACKET_CONTENT_PATTERN = re.compile(r'\\[([^\\]]+)\\]')\n",
    "HTML_TAG_PATTERN = re.compile(r'<[^>]+>')\n",
    "WHITESPACE_PATTERN = re.compile(r'[\\t\\n\\u3000]')\n",
    "MULTI_SPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "# Translation cache\n",
    "translation_cache = {}\n",
    "\n",
    "# Initialize translator once\n",
    "_translator = GoogleTranslator(source='auto', target='zh-CN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77036b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSanitizer:\n",
    "    def __init__(self):\n",
    "        # é¢„ç¼–è¯‘æ‰€æœ‰æ­£åˆ™è¡¨è¾¾å¼\n",
    "        self.mention_pattern = re.compile(r'@\\w+') # @æåŠæ­£åˆ™\n",
    "        self.url_pattern = re.compile(r'(https?://\\S+)') # URLæ­£åˆ™\n",
    "        self.emoji_dict = emoji.EMOJI_DATA  # é¢„åŠ è½½è¡¨æƒ…ç¬¦å·åº“\n",
    "        self.protected_terms = self._load_protected_terms()\n",
    "        \n",
    "    def _load_protected_terms(self):\n",
    "        # ä»æ–‡ä»¶/æ•°æ®åº“åŠ è½½ä¿æŠ¤è¯è¡¨ï¼ˆå“ç‰Œè¯ã€äº§å“è¯ç­‰ï¼‰\n",
    "        return {'é²œèŠ‹ä»™', 'MeetFresh', 'ParkPavilion'}  \n",
    "    \n",
    "    def pipeline(self, text):\n",
    "        \"\"\"å•æ¡æ–‡æœ¬å¤„ç†ç®¡é“\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        text = basic_clean(text)\n",
    "        text = social_media_clean(text)\n",
    "        text = language_optimize(text, protected_terms=self.protected_terms, enable_translation=True)\n",
    "        return text\n",
    "    \n",
    "    def batch_process(self, texts):\n",
    "        \"\"\"æ‰¹é‡å¤„ç†æ–‡æœ¬\"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "            \n",
    "        # è®¡ç®—åˆé€‚çš„æ ¸å¿ƒæ•°\n",
    "        num_cores = max(1, os.cpu_count() * 8 // 10)\n",
    "        chunk_size = max(1, len(texts) // (num_cores * 4))\n",
    "        \n",
    "        # åˆå¹¶ç›¸ä¼¼å¤„ç†æ­¥éª¤ï¼Œå‡å°‘çº¿ç¨‹åˆ‡æ¢\n",
    "        def process_chunk(chunk):\n",
    "            return [self.pipeline(text) for text in chunk]\n",
    "        \n",
    "        # åˆ›å»ºæ–‡æœ¬å—\n",
    "        chunks = [texts[i:i+chunk_size] for i in range(0, len(texts), chunk_size)]\n",
    "        \n",
    "        # å¹¶è¡Œå¤„ç†\n",
    "        with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "            results = list(executor.map(process_chunk, chunks))\n",
    "        \n",
    "        # å±•å¹³ç»“æœ\n",
    "        processed_texts = []\n",
    "        for chunk_result in results:\n",
    "            processed_texts.extend(chunk_result)\n",
    "        \n",
    "        return processed_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b0c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb67ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cooked contents data\n",
    "cont = pd.read_json('..\\..\\Data\\processed\\contents_cooked.json')\n",
    "\n",
    "# combine the title and note_body into a single string\n",
    "def process_text(note):\n",
    "    return note['title'] + ' ' + note['note_body']\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "cont['text'] = cont.apply(process_text, axis=1).astype(str)\n",
    "\n",
    "# Load the MeetFresh user dictionary\n",
    "jieba.load_userdict(\"MF_dict.txt\")\n",
    "\n",
    "# prepare the regex patterns for text processing\n",
    "ZH_CHAR_PATTERN = re.compile(r'[\\u4e00-\\u9fff]')  # ä¸­æ–‡å­—ç¬¦æ£€æµ‹\n",
    "NON_ZH_PATTERN = re.compile(r'[a-zA-Z]')  # æ‹‰ä¸å­—æ¯æ£€æµ‹\n",
    "SPLIT_PATTERN = re.compile(r'([ã€‚ï¼ï¼Ÿ?!.])')  # åˆ†å¥æ­£åˆ™\n",
    "\n",
    "# å¤ç”¨ç¿»è¯‘å™¨å®ä¾‹ï¼ˆé™ä½åˆå§‹åŒ–å¼€é”€ï¼‰\n",
    "_translator = GoogleTranslator(source='auto', target='zh-CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_keywords = {\n",
    "    'é²œèŠ‹ä»™', 'Meet Fresh', 'MeetFresh', 'å°æ¹¾ç¾é£Ÿ', 'ç”œå“', \n",
    "    'èŠ‹åœ†', 'taro', 'ä»™è‰', 'grass jelly', 'å¥¶èŒ¶', 'milk tea',\n",
    "    'è±†èŠ±', 'tofu pudding', 'å¥¶åˆ¨å†°', 'milked shaved ice',\n",
    "    'çº¢è±†æ±¤', 'purple rice soup', 'ç´«ç±³ç²¥', 'red bean soup',\n",
    "    '2001 Coit Rd', 'Park Pavillion Center', '(972) 596-6088',\n",
    "    'é¤å…', 'é¤é¦†', 'ç¾é£Ÿ', 'å°æ¹¾å°åƒ', 'å°æ¹¾ç”œå“', 'å†°æ¿€å‡Œ'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the 1st row of cont\n",
    "cont.iloc[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7090e",
   "metadata": {},
   "source": [
    "ä¸­æ–‡æ–‡æœ¬æ¸…æ´—é»„é‡‘å››æ­¥æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731d2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullwidth_to_halfwidth(text:str) -> str:\n",
    "    \"\"\"å…¨è§’è½¬åŠè§’ï¼ˆä¿ç•™ï¿¥ç¬¦å·ï¼‰\"\"\"\n",
    "    translation_table = str.maketrans({\n",
    "        'ï¼': '!', 'â€œ': '\"', 'â€': '\"', 'â€˜': \"'\", 'â€™': \"'\",\n",
    "        'ã€': ',', 'ï¼Œ': ',', 'ï¼›': ';', 'ï¼š': ':', 'ï¼Ÿ': '?',\n",
    "        'ã€Š': '<', 'ã€‹': '>', 'ã€': '[', 'ã€‘': ']', 'Â·': '.',\n",
    "        'ï½': '~', 'â€”': '-', 'ï¼ˆ': '(', 'ï¼‰': ')', 'ã€€': ' '\n",
    "    })\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "def normalize_punctuation(text:str) -> str:\n",
    "    \"\"\"ç¬¦å·æ ‡å‡†åŒ–ï¼ˆä¿ç•™emoji, retain å­—ç¬¦çš„ä½ç½®ä¿¡æ¯ä½†ç‰ºç‰²äº†æ•ˆç‡, ä¹‹åå¯ä»¥è€ƒè™‘ä¼˜åŒ–\"\"\"\n",
    "    # å®šä¹‰ä¿ç•™ç¬¦å·é›†ï¼ˆæ–°å¢%$ï¿¥ï¼‰\n",
    "    keep_symbols = {\"'\", '\"', ',', '.', ':', ';', '!', '?', '-', \n",
    "                   '(', ')', '<', '>', '[', ']', '&', '#', '@',\n",
    "                   '%', '$', 'ï¿¥', '/', '=', '+', '~', '^'}\n",
    "    \n",
    "    # å­—ç¬¦çº§å¤„ç†, \n",
    "    cleaned_chars = []\n",
    "    for char in text:\n",
    "        # ä¿ç•™æ¡ä»¶ï¼šå­—æ¯æ•°å­—/æ±‰å­—/keep_symbols/emoji\n",
    "        if (char.isalnum() or\n",
    "            '\\u4e00' <= char <= '\\u9fff' or\n",
    "            char in keep_symbols or\n",
    "            emoji.is_emoji(char)):\n",
    "            cleaned_chars.append(char)\n",
    "        else:\n",
    "            cleaned_chars.append(' ')\n",
    "    \n",
    "    return ''.join(cleaned_chars)\n",
    "\n",
    "def remove_urls(text:str) -> str:\n",
    "    \"\"\"é€‚é…ä¸­æ–‡åŸŸåçš„URLç§»é™¤\"\"\"\n",
    "    url_pattern = re.compile(\n",
    "        r'(?:https?://)?(?:[a-zA-Z0-9\\u4e00-\\u9fff-]+\\.)+[a-zA-Z]{2,}'\n",
    "        r'(?:/\\S*)?(?:\\?\\S*)?(?:#\\S*)?',\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "def basic_clean(\n",
    "        text : str\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    å¯¹æ–‡æœ¬è¿›è¡ŒåŸºç¡€å±‚æ¸…æ´—ï¼ŒåŒ…æ‹¬å»é™¤HTMLæ ‡ç­¾ã€URLã€ç‰¹æ®Šç¬¦å·å¤„ç†, ç©ºæ ¼æ ‡å‡†åŒ–ç­‰ã€‚\n",
    "    1. ç§»é™¤HTMLæ ‡ç­¾\n",
    "    2. ç§»é™¤URLï¼ˆé€‚é…ä¸­æ–‡åŸŸåï¼‰\n",
    "    3. å¤„ç†ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™å¸¸ç”¨ç¬¦å·å’Œemoji, å…¨è§’æ ‡ç‚¹è½¬åŠè§’ï¼‰\n",
    "    4. æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦\n",
    "\n",
    "    Args:\n",
    "        text (str): å¾…æ¸…æ´—çš„æ–‡æœ¬ã€‚\n",
    "    Returns:\n",
    "        str: æ¸…æ´—åçš„æ–‡æœ¬ã€‚\n",
    "    \"\"\"\n",
    "    # æ›¿æ¢æ‰€æœ‰ç©ºç™½ç¬¦ï¼ˆå«å°çº¢ä¹¦å¸¸è§çš„å…¨è§’ç©ºæ ¼\\u3000ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ï¼‰\n",
    "    text = re.sub(r'[\\t\\n\\u3000]', ' ', text)\n",
    "    # HTMLæ ‡ç­¾ç§»é™¤\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # URLç§»é™¤, é€‚é…ä¸­æ–‡åŸŸå\n",
    "    text = remove_urls(text)\n",
    "    # å…¨è§’è½¬åŠè§’ï¼ˆä¿ç•™å…¨è§’ï¿¥ï¼‰\n",
    "    text = fullwidth_to_halfwidth(text)\n",
    "    # ç¬¦å·æ ‡å‡†åŒ–\n",
    "    text = normalize_punctuation(text)\n",
    "    # æ ‡å‡†åŒ–åˆå¹¶è¿ç»­ç©ºæ ¼\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587d3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the basic_clean function to the 'text' column\n",
    "cont['text'] = cont['text'].apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd2225c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[è¾¾æ‹‰æ–¯.åƒ]å¿«ä¹å°ç¾Š,å›åˆ°å„¿æ—¶æ¾³é—¨è±†æåŠ Happy Lamb Hot Pot ğŸ“ 240 Legacy Dr ste 116, Plano, TX 75023 æ„Ÿè°¢å¿«ä¹å°ç¾Šçš„é‚€è¯· è™½ç„¶ä¸çŸ¥é“å°ç¾Šå¿«ä¸å¿«ä¹[ç¬‘å“­R]ä½†æˆ‘åƒçš„å¾ˆå¿«ä¹ ç‰¹åˆ«å–œæ¬¢å°ç¾Šå®¶ç¯å¢ƒ,èˆ’é€‚ä½è°ƒä¸å–§å“—,é€‚åˆæœ‹å‹èŠå¤© æ‰€æœ‰èœå“è‡ªå–,æ›´è‡ªç”±å¿«æ·,ç»å¤§å¤šæ•°éƒ½å¾ˆæ–°é²œ(åªæœ‰ä¸ªåˆ«é¹Œé¹‘è›‹é»„ä¸çŸ¥ä¸ºä½•æœ‰ç‚¹å’¸é¸­è›‹å‘³é“,ä¹Ÿè®¸æ˜¯æˆ‘æ•æ„Ÿ[å®³ç¾R])æˆ‘ä»¬é€‰çš„é‡‘æ±¤é…¸è¾£é”…å’Œç‰¹åˆ¶é¦™è¾£é”… é‡‘æ±¤é”…å…¶å®æ›´åƒé…¸èœé”…,é…¸èœå‘³æŒºæµ“,è¿˜æœ‰å¾ˆéº»çš„å£æ„Ÿ,ä¸‹äº†é±¼ç‰‡å’Œç‰›è‚‰,ç§’å˜é…¸èœé±¼å’Œé…¸èœç‰› é¦™è¾£é”…æ˜¯å°æ¹¾å¥åº·æ„Ÿéº»è¾£å°ç«é”…å‘³å„¿,ç›¸å¯¹é‡åº†ç«é”…æ›´ä¸ºæ¸…æ·¡,ä¹Ÿæ›´èƒ½ä½“ç°é£Ÿææœ¬èº«çš„å‘³é“ å¼ºçƒˆæ¨èä»–å®¶è‡»å“ç¾Šè‚‰ç‰‡,è‚¥ç˜¦åˆé€‚è¿˜æœ‰å¥¶é¦™å‘³ ç„¶åæ‰‹æ‰“ç¾Šè‚‰ä¸¸,å’Œé¦™èœæ··åˆåˆ¶ä½œ,ç‰¹åˆ«å¥½åƒ å°æ¹¾é…¸ç”œå£åŒ…å¿ƒèœæ³¡èœæ°¸è¿œåƒä¸è…» ä½œä¸ºç¢³æ°´å¤§æˆ·,ç‚’é¥­ç‚’é¢ç‚¸é¦’å¤´ç‚¸éº»çƒå®Œå…¨æ»¡è¶³äº†æˆ‘çš„éœ€æ±‚,ç‚¸é¸¡ç¿…å’Œæ©™å­ä¹Ÿå¾ˆä¸é”™,å°æœ‹å‹é¥­åº”æœ‰å°½æœ‰~ é‡ç‚¹æ¥äº†!å°ç¾Šç»™çš„ä¸¤æ¯é¸¡å°¾é…’æŠŠä¸å¤ªå–é…’çš„æˆ‘ç»™æƒŠè‰³äº†!å›¾10ä¸­çŸ®çš„é‚£æ¯æ¸…å†½çš„é…’ç²¾å¸¦ç€ä¸ä¸ç”œå‘³,é†‡é¦™å£æ„Ÿå¹²è„†åˆ©è½,ä¼šä¸€ç›´æƒ³å–ä¸åœ æœ‰æŸ æª¬ç‰‡çš„é‚£æ¯æ˜¯ä¸€ç§æ··åˆæœæ±é…ç€æ¸…æ·¡é…¸å¥¶æ³¡çš„æ— é…’ç²¾é¸¡å°¾é…’,æ‰“è´¥æˆ‘çˆ±çš„æ‰€æœ‰æœæ± æ²¡æƒ³åˆ°å°ç¾Šè¦é é¸¡å°¾é…’å‡ºé“äº†[ç¬‘å“­R] ä¸€é¡¿é¥­ä¸‹æ¥,ä¸ä»…åƒçš„æ»¡è¶³,æ•´ä¸ªæ°›å›´ä¹Ÿè®©äººæƒ³èµ·ç«¥å¹´å›å¿†é‡Œçš„æ¾³é—¨è±†æåŠ,å¹²å‡€æƒ¬æ„æœ‰æƒ…è°ƒ æˆ‘å°±å–œæ¬¢è¿™æ ·å®‰å®‰é™é™äº«å—ç«é”…çš„å¿«ä¹ #è¾¾æ‹‰æ–¯ç«é”…[è¯é¢˜]# #è¾¾æ‹‰æ–¯ç¾é£Ÿ[è¯é¢˜]# #è¾¾æ‹‰æ–¯ç”Ÿæ´»[è¯é¢˜]# #è¾¾æ‹‰æ–¯[è¯é¢˜]# #è¾¾æ‹‰æ–¯æ¢åº—[è¯é¢˜]# #è¾¾æ‹‰æ–¯å‘¨è¾¹[è¯é¢˜]# #è¾¾æ‹‰æ–¯å‘¨æœ«[è¯é¢˜]# #è¾¾æ‹‰æ–¯åƒå–ç©ä¹[è¯é¢˜]# #å¿«ä¹å°ç¾Š[è¯é¢˜]# #å¿«ä¹å°ç¾Šç«é”…[è¯é¢˜]#'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont['text'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d81f2",
   "metadata": {},
   "source": [
    "ç¤¾äº¤åª’ä½“ç‰¹å¾å¤„ç†å±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b77ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_media_clean(text:str, strategy='demojize') -> str:\n",
    "    \"\"\"\n",
    "    ç¤¾äº¤åª’ä½“æ–‡æœ¬æ¸…æ´—ï¼Œä¸»è¦é’ˆå¯¹å°çº¢ä¹¦å¹³å°çš„ç‰¹å®šæ ¼å¼å’Œç¬¦å·è¿›è¡Œå¤„ç†ã€‚\n",
    "    1. ç§»é™¤è¯é¢˜æ ‡ç­¾ï¼ˆ#ï¼‰ä½†ä¿ç•™å…³é”®è¯\n",
    "    2. ç§»é™¤@æåŠ, support ä¸­è‹±æ–‡å¤åˆç”¨æˆ·å\n",
    "    3. è½¬æ¢è¡¨æƒ…ç¬¦å·ï¼ˆå¯é€‰ï¼šç§»é™¤æˆ–è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼‰\n",
    "    4. å¤„ç†å°çº¢ä¹¦ç‰¹æœ‰æ–¹æ‹¬å·\n",
    "    5. å»é™¤å¤šä½™ç©ºæ ¼\n",
    "\n",
    "    Args:\n",
    "        text (str): å¾…æ¸…æ´—çš„æ–‡æœ¬ã€‚\n",
    "    Returns:\n",
    "        str: æ¸…æ´—åçš„æ–‡æœ¬ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # ç§»é™¤è¯é¢˜æ ‡ç­¾ä½†ä¿ç•™å…³é”®è¯ï¼ˆå¦‚ #è¾¾æ‹‰æ–¯ç¾é£Ÿ# â†’ è¾¾æ‹‰æ–¯ç¾é£Ÿï¼‰\n",
    "    text = re.sub(r'#([^#]+)#', r'\\1', text)\n",
    "    \n",
    "    # ç§»é™¤@æåŠ(åŒ…å«å…¶å˜ä½“, å¦‚@å°çº¢ä¹¦ç”¨æˆ·)\n",
    "    text = re.sub(r'@[\\w\\u4e00-\\u9fff-]+', '', text)  # æ”¯æŒä¸­è‹±æ–‡ç”¨æˆ·å\n",
    "    \n",
    "    # è½¬æ¢Emojiï¼ˆå¯é€‰ç­–ç•¥ï¼‰\n",
    "    if strategy == 'remove':\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "    elif strategy == 'demojize':\n",
    "        # å°†emojiè½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼ˆå¦‚:ğŸ˜€ â†’ :grinning_face:ï¼‰\n",
    "        text = emoji.demojize(text, delimiters=(' [', '] '))\n",
    "    \n",
    "    # å¤„ç†å°çº¢ä¹¦ç‰¹æœ‰çš„æ–¹æ‹¬å·æ ‡ç­¾ï¼ˆåˆ é™¤ç³»ç»Ÿæ ‡ç­¾, å¦‚[è¯é¢˜]â†’'')\n",
    "    text = re.sub(r'\\[è¯é¢˜\\s*[^\\]]*\\]', '', text)  # åˆ é™¤ç³»ç»Ÿæ ‡ç­¾ \n",
    "\n",
    "    # ç»§ç»­å¤„ç†å°çº¢ä¹¦æ–¹æ‹¬å·æ ‡ç­¾ï¼ˆåŒæ—¶ä¿ç•™å…³é”®æ–‡æœ¬ä¿¡æ¯, å¦‚[ç¬‘å“­R]â†’ç¬‘å“­Rï¼‰\n",
    "    text = re.sub(r'\\[(?:è¡¨æƒ…|åœ°ç‚¹)\\s*[^\\]]*\\]', '', text)\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]', r'\\1', text)  # å»é™¤æ–¹æ‹¬å·ä½†ä¿ç•™å†…å®¹\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10bd075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the social_media_clean function to the 'text' column\n",
    "cont['text'] = cont['text'].apply(social_media_clean, strategy='demojize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cdd92",
   "metadata": {},
   "source": [
    "ä¸­æ–‡è‹±æ–‡æ··åˆè¯­è¨€ç¯å¢ƒä¼˜åŒ–å±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b1efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_of_chinese(text: str) -> float:\n",
    "    \"\"\"è¿”å› text ä¸­ï¼ˆUnicodeèŒƒå›´4E00-9FFFï¼‰æ±‰å­—çš„å æ¯”ã€‚\"\"\"\n",
    "    if not text:\n",
    "        return 1.0\n",
    "    zh_chars = ZH_CHAR_PATTERN.findall(text)\n",
    "    return len(zh_chars) / len(text) if len(zh_chars) > 0 else 0.0\n",
    "\n",
    "def mask_protected_terms(text: str, protected_terms: set) -> (str, dict):\n",
    "    \"\"\"\n",
    "    å°† text ä¸­å‡ºç°çš„ä¿æŠ¤è¯ç”¨å ä½ç¬¦æ›¿æ¢ï¼Œå¹¶è¿”å›æ›¿æ¢åçš„æ–‡æœ¬ä»¥åŠå ä½ç¬¦æ˜ å°„å­—å…¸ã€‚\n",
    "    æ¯”å¦‚ \"MeetFresh\" -> \"[EN_TERM_0]\"\n",
    "    \"\"\"\n",
    "    if not protected_terms:\n",
    "        return text, {}\n",
    "    \n",
    "    # æŒ‰æœ¯è¯­é•¿åº¦é™åºæ’åˆ—ï¼ˆä¼˜å…ˆåŒ¹é…é•¿è¯ï¼‰\n",
    "    sorted_terms = sorted(protected_terms, key=lambda x: len(x), reverse=True)\n",
    "    pattern = re.compile(\n",
    "        r'\\b(' + '|'.join(map(re.escape, sorted_terms)) + r')\\b', \n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    placeholder_map = {}\n",
    "    idx = 0\n",
    "    \n",
    "    def _repl(m):\n",
    "        nonlocal idx\n",
    "        placeholder = f\"[EN_TERM_{idx}]\"\n",
    "        placeholder_map[placeholder] = m.group(0)\n",
    "        idx += 1\n",
    "        return placeholder\n",
    "    \n",
    "    return pattern.sub(_repl, text), placeholder_map\n",
    "\n",
    "def unmask_protected_terms(text: str, placeholder_map: dict) -> str:\n",
    "    \"\"\"å°†ç¿»è¯‘åæ–‡æœ¬ä¸­çš„å ä½ç¬¦æ¢å¤æˆåŸå§‹è‹±æ–‡æœ¯è¯­\"\"\"\n",
    "    for placeholder, original in placeholder_map.items():\n",
    "        text = text.replace(placeholder, original)\n",
    "    return text\n",
    "\n",
    "@lru_cache(maxsize=5010)\n",
    "def cached_translate(text: str) -> str:\n",
    "    \"\"\"å¸¦ç¼“å­˜çš„ç¿»è¯‘æ–¹æ³•ï¼Œæ•´åˆäº†å®‰å…¨æ£€æŸ¥\"\"\"\n",
    "    # 1. ç©ºå­—ç¬¦ä¸²æˆ–ä»…åŒ…å«ç©ºæ ¼ã€æ ‡ç‚¹ã€æ•°å­—ç­‰ï¼Œå¯ç›´æ¥è¿”å›åŸæ–‡\n",
    "    if not text or text.strip().isdigit():\n",
    "        return text \n",
    "    \n",
    "    # 2. é¿å…è¶…è¿‡ 5000 å­—ç¬¦çš„æ–‡æœ¬\n",
    "    if len(text) > 5000:\n",
    "        print(f\"Warning: text too long ({len(text)} chars). Truncating for translation.\")\n",
    "        text = text[:4900]  # é€‚å½“æˆªæ–­ä»¥é˜²APIé™åˆ¶\n",
    "\n",
    "    # 3. è°ƒç”¨ç¿»è¯‘ï¼Œå¹¶æ•æ‰å¼‚å¸¸\n",
    "    try:\n",
    "        return _translator.translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation failed: {e}\")\n",
    "        return text\n",
    "\n",
    "def language_optimize(\n",
    "    text: str,\n",
    "    threshold: float = 0.5,\n",
    "    protected_terms: set = None,\n",
    "    enable_translation: bool = True\n",
    ") -> str:\n",
    "    \"\"\"ç»ˆæä¼˜åŒ–ç‰ˆè¯­è¨€å¤„ç†\"\"\"\n",
    "    if not enable_translation or not text:\n",
    "        return text\n",
    "    \n",
    "    if protected_terms is None:\n",
    "        protected_terms = {\n",
    "            \"MeetFresh\", \"VIP\", \"AI\", \"DFW\", \n",
    "            \"Grass Jelly\", \"Taro\", \"Milk Tea\",\n",
    "            \"Red Bean Soup\", \"Purple Rice Soup\",\n",
    "            \"Tofu Pudding\", \"Shaved Ice\",\n",
    "            \"Purple Rice\", '2001 Coit Rd', 'Park Pavillion Center'\n",
    "        }\n",
    "    \n",
    "    # é¢„å¤„ç†ï¼šå¿«é€Ÿè¿‡æ»¤çº¯ä¸­æ–‡æ–‡æœ¬æˆ–ç©ºæ–‡æœ¬\n",
    "    if not text or not NON_ZH_PATTERN.search(text):\n",
    "        return text\n",
    "    \n",
    "    # åˆ†å¥ä¼˜åŒ–ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰\n",
    "    buffer = []\n",
    "    segments = SPLIT_PATTERN.split(text)\n",
    "    \n",
    "    for i in range(0, len(segments), 2):\n",
    "        sentence = segments[i]\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # å¿«é€Ÿè·³è¿‡çº¯ä¸­æ–‡å¥\n",
    "        if NON_ZH_PATTERN.search(sentence):\n",
    "            # å…ˆä¿æŠ¤é‡è¦æœ¯è¯­\n",
    "            masked, ph_map = mask_protected_terms(sentence, protected_terms)\n",
    "            \n",
    "            # åªç¿»è¯‘ä¸­æ–‡å æ¯”ä½äºé˜ˆå€¼çš„å¥å­\n",
    "            if ratio_of_chinese(masked) < threshold:\n",
    "                translated = cached_translate(masked)\n",
    "                unmasked = unmask_protected_terms(translated, ph_map)\n",
    "                buffer.append(unmasked)\n",
    "            else:\n",
    "                buffer.append(sentence)\n",
    "        else:\n",
    "            buffer.append(sentence)\n",
    "        \n",
    "        # æ·»åŠ åˆ†éš”ç¬¦\n",
    "        if i+1 < len(segments):\n",
    "            buffer.append(segments[i+1])\n",
    "    \n",
    "    return ''.join(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b5a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the language_optimize function to the 'text' column\n",
    "cont['text'] = cont['text'].apply(language_optimize, enable_translation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737d34d",
   "metadata": {},
   "source": [
    "é«˜çº§è¯­ä¹‰æ¸…æ´—å±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1d5c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_semantic_clean(\n",
    "    texts: List[str],\n",
    "    freq_threshold: float = 0.25,          # è¯é¢‘é˜ˆå€¼\n",
    "    doc_freq_threshold: float = 0.6,       # æ–‡æ¡£é¢‘ç‡é˜ˆå€¼\n",
    "    min_word_length: int = 2,              # æœ€å°è¯é•¿åº¦ï¼ˆè¿‡æ»¤å•å­—è¯ï¼‰\n",
    "    custom_stopwords: Optional[Set[str]] = None,\n",
    "    domain_keywords: Optional[Set[str]] = None,\n",
    "    return_noise_terms: bool = False       # æ˜¯å¦è¿”å›è¯†åˆ«å‡ºçš„å™ªå£°è¯\n",
    ") -> Union[List[str], tuple]:\n",
    "    \"\"\"\n",
    "    åŸºäºè¯é¢‘ç»Ÿè®¡çš„ä¸­æ–‡è¯­ä¹‰æ¸…æ´—å‡½æ•°\n",
    "    \n",
    "    Args:\n",
    "        texts: å¾…æ¸…æ´—çš„æ–‡æœ¬åˆ—è¡¨\n",
    "        freq_threshold: è¯é¢‘é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼çš„è¯è¢«è§†ä¸ºå™ªå£°ï¼ˆé™¤éåœ¨domain_keywordsä¸­ï¼‰\n",
    "        doc_freq_threshold: æ–‡æ¡£é¢‘ç‡é˜ˆå€¼ï¼Œå‡ºç°åœ¨è¶…è¿‡æ­¤æ¯”ä¾‹æ–‡æ¡£ä¸­çš„è¯è¢«è§†ä¸ºå™ªå£°\n",
    "        min_word_length: æœ€å°è¯é•¿åº¦ï¼Œå°äºæ­¤é•¿åº¦çš„è¯ä¸å‚ä¸ç»Ÿè®¡\n",
    "        custom_stopwords: è‡ªå®šä¹‰åœç”¨è¯é›†åˆ\n",
    "        domain_keywords: é¢†åŸŸå…³é”®è¯é›†åˆï¼ˆè¿™äº›è¯ä¸ä¼šè¢«è¿‡æ»¤ï¼‰\n",
    "        return_noise_terms: æ˜¯å¦è¿”å›è¯†åˆ«å‡ºçš„å™ªå£°è¯\n",
    "        \n",
    "    Returns:\n",
    "        æ¸…æ´—åçš„æ–‡æœ¬åˆ—è¡¨ï¼Œå¦‚æœreturn_noise_termsä¸ºTrueï¼Œåˆ™åŒæ—¶è¿”å›è¯†åˆ«çš„å™ªå£°è¯é›†åˆ\n",
    "    \"\"\"\n",
    "    # é»˜è®¤å°çº¢ä¹¦å¸¸è§å™ªå£°è¯\n",
    "    default_stopwords = {\n",
    "        # æƒ…æ„Ÿå¼ºåŒ–è¯\n",
    "        \"çœŸçš„\", \"çœŸæ˜¯\", \"å¤ª\", \"å¥½\", \"å¾ˆ\", \"éå¸¸\", \"è¶…çº§\", \"ç»å¯¹\", \"ç®€ç›´\",\n",
    "        # ç½‘ç»œç”¨è¯­\n",
    "        \"å“ˆå“ˆ\", \"å“ˆå“ˆå“ˆ\", \"å•Šå•Š\", \"å•Šå•Šå•Š\", \"å‘œå‘œ\", \"å‘œå‘œå‘œ\", \"omg\", \"OMG\",\n",
    "        \"xswl\", \"awsl\", \"yyds\", \"ç»ç»å­\", \"æ— è¯­å­\",\n",
    "        # å£å¤´ç¦…\n",
    "        \"çœŸçš„æ˜¯\", \"å°±æ˜¯\", \"åæ­£\", \"ç„¶å\", \"å…¶å®\", \"é‚£ä¸ª\", \"è¿™ä¸ª\", \"æ‰€ä»¥\",\n",
    "        \"emmm\", \"emm\", \"å•Šè¿™\", \"è¹²ä¸€ä¸ª\", \"å†²é¸­\",\n",
    "        # æ ‡ç‚¹ç¬¦å·ç»„åˆ\n",
    "        \"ï½ï½\", \"â€¦\"\n",
    "    }\n",
    "    \n",
    "    # åˆå¹¶è‡ªå®šä¹‰åœç”¨è¯\n",
    "    stopwords = default_stopwords.copy()\n",
    "    if custom_stopwords:\n",
    "        stopwords.update(custom_stopwords)\n",
    "    \n",
    "    # åˆå§‹åŒ–é¢†åŸŸå…³é”®è¯é›†åˆ\n",
    "    domain_keywords = domain_keywords or set()\n",
    "    \n",
    "    # ç»Ÿè®¡è¯é¢‘å’Œæ–‡æ¡£é¢‘ç‡\n",
    "    total_docs = len(texts)\n",
    "    word_counts = Counter()\n",
    "    doc_counts = defaultdict(int)\n",
    "    \n",
    "    print(f\"æ­£åœ¨ç»Ÿè®¡è¯é¢‘ï¼ˆå…±{total_docs}ç¯‡æ–‡æœ¬ï¼‰...\")\n",
    "    for text in texts:\n",
    "        words = jieba.lcut(text)\n",
    "        # è¿‡æ»¤å¤ªçŸ­çš„è¯\n",
    "        words = [w for w in words if len(w) >= min_word_length]\n",
    "        \n",
    "        # æ›´æ–°å…¨å±€è¯é¢‘\n",
    "        word_counts.update(words)\n",
    "        \n",
    "        # æ›´æ–°æ–‡æ¡£é¢‘ç‡ï¼ˆæ¯ä¸ªæ–‡æ¡£ä¸­çš„è¯åªè®¡ç®—ä¸€æ¬¡ï¼‰\n",
    "        unique_words = set(words)\n",
    "        for word in unique_words:\n",
    "            doc_counts[word] += 1\n",
    "    \n",
    "    # è®¡ç®—ç›¸å¯¹é¢‘ç‡\n",
    "    word_freq = {word: count/total_docs for word, count in word_counts.items()}\n",
    "    doc_freq = {word: count/total_docs for word, count in doc_counts.items()}\n",
    "    \n",
    "    # è¯†åˆ«å™ªå£°è¯ï¼ˆé«˜é¢‘ä½†ä¸æ˜¯é¢†åŸŸå…³é”®è¯ï¼‰\n",
    "    noise_terms = {\n",
    "        word for word, freq in word_freq.items()\n",
    "        if (freq > freq_threshold or  # å…¨å±€é«˜é¢‘\n",
    "            doc_freq[word] > doc_freq_threshold) and  # æ–‡æ¡£é«˜é¢‘\n",
    "           word not in domain_keywords  # ä¸æ˜¯é¢†åŸŸå…³é”®è¯\n",
    "    }\n",
    "    \n",
    "    # åˆå¹¶è‡ªå®šä¹‰åœç”¨è¯\n",
    "    noise_terms.update(stopwords)\n",
    "    \n",
    "    print(f\"å·²è¯†åˆ«å™ªå£°è¯{len(noise_terms)}ä¸ªï¼Œå¼€å§‹æ¸…æ´—...\")\n",
    "    \n",
    "    # æ„å»ºè¿‡æ»¤æ¨¡å¼\n",
    "    pattern = re.compile('|'.join(noise_terms))\n",
    "    cleaned_texts = [pattern.sub('', text) for text in texts]\n",
    "    \n",
    "    # è¯„ä¼°æ•´ä½“æ•ˆæœ\n",
    "    total_original_length = sum(len(t) for t in texts)\n",
    "    total_cleaned_length = sum(len(t) for t in cleaned_texts)\n",
    "    compression_ratio = total_cleaned_length / total_original_length\n",
    "    \n",
    "    print(f\"æ¸…æ´—å®Œæˆ! å™ªå£°å»é™¤ç‡: {(1-compression_ratio):.2%}\")\n",
    "    print(f\"åŸå§‹æ€»å­—ç¬¦æ•°: {total_original_length}\")\n",
    "    print(f\"æ¸…æ´—åæ€»å­—ç¬¦æ•°: {total_cleaned_length}\")\n",
    "    \n",
    "    if return_noise_terms:\n",
    "        return cleaned_texts, noise_terms\n",
    "    \n",
    "    return cleaned_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63edad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the chinese_semantic_clean function to the 'text' column\n",
    "cont['text'] = chinese_semantic_clean(\n",
    "    cont['text'].tolist(),\n",
    "    freq_threshold=0.25,\n",
    "    doc_freq_threshold=0.6,\n",
    "    min_word_length=2,\n",
    "    custom_stopwords=None,\n",
    "    domain_keywords=domain_keywords,\n",
    "    return_noise_terms=False\n",
    ")\n",
    "# è¯­ä¹‰æ¸…æ´—åï¼Œå»é™¤ç©ºæ–‡æœ¬\n",
    "cont = cont[cont['text'].str.strip() != '']\n",
    "#cont = cont.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175e156",
   "metadata": {},
   "source": [
    "å…¨æµç¨‹é›†æˆç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4609cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSanitizer:\n",
    "    def __init__(self):\n",
    "        self.mention_pattern = re.compile(r'@\\w+') # @æåŠæ­£åˆ™\n",
    "        self.url_pattern = re.compile(r'(https?://\\S+)') # URLæ­£åˆ™\n",
    "        self.emoji_dict = emoji.EMOJI_DATA  # é¢„åŠ è½½è¡¨æƒ…ç¬¦å·åº“\n",
    "        self.protected_terms = self._load_protected_terms()\n",
    "        \n",
    "    def _load_protected_terms(self):\n",
    "        # ä»æ–‡ä»¶/æ•°æ®åº“åŠ è½½ä¿æŠ¤è¯è¡¨ï¼ˆå“ç‰Œè¯ã€äº§å“è¯ç­‰ï¼‰\n",
    "        return {'é²œèŠ‹ä»™', 'MeetFresh', 'ParkPavilion'}  \n",
    "    \n",
    "    def pipeline(self, text):\n",
    "        text = basic_clean(text)\n",
    "        text = social_media_clean(text)\n",
    "        text = language_optimize(text, protected_terms=self.protected_terms, enable_translation=True)\n",
    "        text = language_optimize(text)\n",
    "        return text\n",
    "    \n",
    "    def batch_process(self, texts):\n",
    "        # å¹¶è¡ŒåŠ é€Ÿï¼ˆåˆ©ç”¨80%CPUæ ¸å¿ƒï¼‰\n",
    "        num_cores = os.cpu_count() * 8 // 10\n",
    "        # ç¡®ä¿è‡³å°‘ä½¿ç”¨ä¸€ä¸ªæ ¸å¿ƒ\n",
    "        num_cores = max(1, num_cores)\n",
    "        # ä½¿ç”¨ProcessPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†\n",
    "        with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "            return list(executor.map(self.pipeline, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35937f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®ä¾‹åŒ– TextSanitizer ç±»\n",
    "sanitizer = TextSanitizer()\n",
    "# æ‰¹é‡å¤„ç†æ–‡æœ¬\n",
    "cont['text'] = sanitizer.batch_process(cont['text'].tolist())\n",
    "# è¯­ä¹‰æ¸…æ´—åï¼Œå»é™¤ç©ºæ–‡æœ¬\n",
    "cont = cont[cont['text'].str.strip() != '']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
